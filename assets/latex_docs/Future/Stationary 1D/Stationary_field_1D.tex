\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
%PACKAGES
\usepackage{CJKutf8}
\usepackage[colorlinks = true,
    linkcolor = blue,
    urlcolor  = black,
    citecolor = blue,
    anchorcolor = blue]{hyperref}
\usepackage[T1]{fontenc}
\makeatletter
\def\ps@pprintTitle{%
    \let\@oddhead\@empty
    \let\@evenhead\@empty
    \let\@oddfoot\@empty
    \let\@evenfoot\@oddfoot
}
\usepackage{amssymb,amsmath,physics,amsthm,xcolor,graphicx}
\usepackage[shortlabels]{enumitem}
\newtheorem{observation}{Observation}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newcommand{\red}[1]{{\color{red}#1}}
\usepackage[colorlinks = true,
    linkcolor = blue,
    urlcolor  = black,
    citecolor = blue,
    anchorcolor = blue]{hyperref}
\usepackage{cleveref}
\bibliographystyle{elsarticle-num}\newcommand{\Ck}{\mathcal{K}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\A}{\mathbb{A}}\newcommand{\C}{\mathbb{C}}\newcommand{\E}{\mathbb{E}}\newcommand{\F}{\mathbb{F}}\newcommand{\K}{\mathbb{K}}\newcommand{\LL}{\mathbb{L}}\newcommand{\M}{\mathbb{M}}\newcommand{\N}{\mathbb{N}}\newcommand{\PP}{\mathbb{P}}\newcommand{\Q}{\mathbb{Q}}\newcommand{\R}{{\mathbb R}}\newcommand{\T}{{\mathbb T}}\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Ww}{\mathcal{W}}\newcommand{\Aa}{\mathcal{A}}\newcommand{\Bb}{\mathcal{B}}\newcommand{\Cc}{\mathcal{C}}\newcommand{\Ee}{\mathcal{E}}\newcommand{\Ff}{\mathcal{F}}\newcommand{\Gg}{\mathcal{G}}\newcommand{\Hh}{\mathcal{H}}\newcommand{\Kk}{\mathcal{K}}\newcommand{\Ll}{\mathcal{L}}\newcommand{\Mm}{\mathcal{M}}\newcommand{\Nn}{\mathcal{N}}\newcommand{\Pp}{\mathcal{P}}\newcommand{\Qq}{\mathcal{Q}}\newcommand{\Rr}{{\mathcal R}}\newcommand{\Ss}{{\mathcal S}}\newcommand{\Tt}{{\mathcal T}}\newcommand{\Zz}{{\mathcal Z}}\newcommand{\Uu}{{\mathcal U}}
\newcommand\restr[2]{{\left.\kern-\nulldelimiterspace #1\vphantom{\big|} \right|_{#2}}}
\newcommand{\br}[1]{\left\langle#1\right\rangle}
\pagestyle{empty}
\setlength{\parindent}{0in}

\begin{document}
\section{Introduction}

The goal of this post is to introduce the theory of \emph{stationary random fields}. In spatio-temporal statistics the typical setup is one wants to model some random quantity $X$ which takes
different values at different positions in time and space. That is, one has a collection of random variables $\{X(u)\}_{u\in I}$ over some index $I$. Typically  $I$ is some subset
of the temporal domain $\R_+$ (temporal fields), of a spatial domain $\R^d$ (spatial fields) or a subset of their product $\R_+\times\R^d$ (space-time fields). But it can also be something more exotic like
a manifold or a Hilbert space.
\\\\
As for what $X(u)$ actually represents it could be anything
that interests us and that fundamentally has some uncertainty. For example  $X$ could be a weather field, the concentration of a chemical in the stream next to your house, the amount of moose in the
Amazon rainforest or even be elements of a Hilbert space. \\
\\
In this series of posts we'll show the relationship between stationary fields and Fourier analysis through Bochner's theorem and the spectral theorem (links). Furthermore we'll show how differential differential equations (both SDEs and SPDEs) with constant coefficients have as solutions stationary fields. This fact is very useful both for modeling and numerical purposes.Ok enough chit-chat, time for some definitions that we'll use throughout these posts.

\subsection{Preliminary definitions and notation}
We consider in what follows a complete probability space $(\Omega,\Ff,\mathbb{P})$. We will also fix a Banach space $E$ and a complex Hilbert space $H$ with inner product $\br{\cdot,\cdot}$.
\begin{definition}
    Given a measurable space $(N,\Nn)$, we say that $X:\Omega\to N$ is a \emph{$N$-valued random variable} if  $X$ is  $\Ff/\Nn$  measurable.
\end{definition}
The above definition depends on the sigma-algebra $\Nn$ placed on $N$. In our work, $N$ will only ever be a topological space. In this case, we will implicitly assume that $\Nn=\Bb(N)$ is the Borel sigma-algebra (link) on $N$.
\begin{definition}
    Given a measurable space $(N,\Nn)$ and an arbitrary set $I$  we say that a \emph{$N$ valued random field with index set $I$} is a collection $\{X(u)\}_{u\in I} $ of $N$-valued random variables.
\end{definition}
We will abbreviate the above to say that $X: I\to N$ is a (random) field. Furthermore, in the case where the $I$ and  $N$ are understood to be fixed, we will simply say that $X$ is a field. We also note that the above measurability condition is equivalent to requiring that $X$ is a  $N^I$ valued random variable (see for example \cite{ccinlar2011probability} page 45).
\\
\\
Given a measure space $(M,\Mm,\mu )$ and a Banach space $E$, we write\\
$L^p(M\to E,\mu)$ for the space of equivalence classes of $p$-integrable functions to  $E$ with the norm
\begin{equation*}
    \norm{f}_{L^p(M\to E)}:=\qty(\int_{\Omega}\norm{f}^p_E d\mu)^\frac{1}{p}.
\end{equation*}
In practice, we will almost always take $E=\R^n$ or $E=\C^n$. In the case where $E=\R$ we will abbreviate $L^p(M\to\R,\mu)$ to $L^p(M,\mu)$. Furthermore, if the measure space is $(\Omega,\mathcal{F},\mathbb{P})$ or $(\Omega,\mathcal{B}(\R^d),\lambda^d)$ where $\lambda^d$ is the Lebesgue measure on $\R^d$ we will not specify the measure. Writing for example
\begin{equation*}
    L^p(\R^d\to\C^n):=L^p(\R^d\to\C^n,\lambda^d);\quad L^p(\Omega\to\C^n):=L^p(\Omega\to\C^n,\PP).
\end{equation*}
\begin{definition}
    Given a  random field $X:I\to E$, we say that $X$ is a \emph{$p$-integrable field} if $X(u)\in L^p(\Omega\to E)$ for all $u\in I$.
\end{definition}
Finally, given $f\in L^1(\R^d\to\C^n)$, we will write
\begin{equation}\label{ft def}
    \hat{f}(\omega):=\qty(\frac{1}{2\pi})^d\int_{\R^d} f(u)e^{i\omega\cdot  u}du;\quad\check{f}(\omega):=\int_{\R^d} f(u)e^{i\omega\cdot  u}du,
\end{equation}
We recall that the Fourier transform and its inverse extend continuously to isometries on $L^2(\R^d\to\C^n)$ (which we denote with the same notation $\hat{\cdot},\check{\cdot}$) with
\begin{equation}
    \check{\hat{f}}=\hat{\check{f}}=f\quad\forall f\in L^2(\R^d\to\C^n).
\end{equation}
See for example \cite{taylor2013partial} pages 222-226 for a proof.
\section{The $1$ dimensional case}\label{1d}
To begin with, we will consider a $\C$ valued random field $X$ with index set $\R^d$ and then build the multidimensional case of $\C^n$ and even Hilbert valued random field from there.
\begin{definition}
    Given a square-integrable random field $X:\R^d\to \C$ we define the \emph{covariance function} of $X$ as
    \begin{equation*}
        C(u,v):=\mathbb{E}[X(u)\overline{X(v)}]-\mathbb{E}[X(u)]\overline{\mathbb{E}[X(v)]}
    \end{equation*}
\end{definition}
\begin{definition}
    Given a square integrable random field $X:\R^d\to\C$  we say that $X$ is \emph{weakly stationary} if for all $u,v\in \R^d$
    \begin{equation*}
        \E[X(u)]=\E[X(v)];\quad r(u-v)=C(u,v).
    \end{equation*}
    For some function $r:\R\to \C$. By abuse of notation, we will also call $r$ the covariance function of $X$.
\end{definition}
That is, a field is weakly stationary if the covariance between the realizations of the field at two different points only depends on the difference between the points.\\
\\
The use of weak stationarity is so widespread that it is usually abbreviated to just saying $X$ is stationary and we will use that convention here. That said, a calculation shows that for any (weakly) stationary stochastic field its covariance function must be a Hermitian function, that is,
\begin{equation*}
    \overline{r(u)}=r(-u).
\end{equation*}

As we will see, the covariance function is one of the fundamental objects of stationary fields, we now discuss its properties.
\begin{definition}
    A linear operator $T: H\to H$ of a Hilbert space is said to be positive semi-definite if
    \begin{equation*}
        \br{Tx,x}\geq 0 \quad\forall x \in H.
    \end{equation*}
    Where $x^\dagger$ is the adjoint transpose of $x$.
\end{definition}
Since every operator between finite dimensional vector spaces is given by a matrix we use the following definition
\begin{definition}
    We say that a matrix $A \in \C^{n\times n}$ is positive and semi-definite if
    \begin{equation*}
        \br{Ax,x}=x^\dagger Ax\geq 0\quad\forall x \in \C^n.
    \end{equation*}
\end{definition}
One example of a positive semi-definite function is the covariance matrix of a $\C^n$ valued random variable  $Y$. That is, $$A_{ij}=Cov(Y_i,\overline{Y_j})=(YY^\dagger-\mathbb{E}[Y]\mathbb{E}[Y^\dagger])_{ij}$$ is positive definite as it holds that given $x\in \C^n$
\begin{equation*}
    x^\dagger\qty(\E[YY^\dagger]-\mathbb{E}[Y]\mathbb{E}[Y^\dagger])x=\E\left[\norm{Y^\dagger x}^2\right]-\mathbb{E}\qty[(Y^\dagger x)^\dagger]\mathbb{E}\qty[Y^\dagger x]\geq 0.
\end{equation*}
Where the last inequality is due to Cauchy-Schwartz.
In particular, the covariance matrix $(C(u_i,u_j))_{i,j=1}^n$ of any stochastic field is positive semi-definite for all $n\in\mathbb{N}$. If the field is stationary then, by definition,
$$r(u_i-u_j)=C(u_i,u_j),$$
so $(r(u_i-u_j))_{i,j=1}^n$ is positive semi-definite for any  $n\in\N$ and for any $u_1,\ldots,u_n \in \R^d$.
This leads to the following definition.
\begin{definition}
    We say that $r:\R^d\to \C$ is positive semi-definite if the matrix  $(r(u_i-u_j))_{i,j=1}^n$ is positive semi-definite for any  $n\in\N$ and for any $u_1,\ldots,u_n \in \R^d$. That is, if
    \begin{equation*}
        \sum_{j,k=1}^{n} r(u_j-u_k)a_j\overline{a}_k\geq 0 \quad\forall a_j \in \C, u_j\in \R^d.
    \end{equation*}
\end{definition}


Since the above sum must be real, a small reasoning  shows that any positive semi-definite function must be Hermitian. Thus, it is reasonable to inquire whether every positive semi-definite function is the covariance function of some field.
\begin{proposition}
    There is a $1$ to  $1$ equivalence between positive semi-definite functions and  covariance functions of weakly stationary fields.
\end{proposition}
\begin{proof}
    We have already seen that every covariance function is positive semi-definite, so it remains to check that a positive semi-definite function $r(u)$ corresponds to the  covariance function of some weakly stationary field. We can choose the process to be Gaussian. This follows by the fact that given any positive semi-definite Hermitian matrix $A$,  we can always find a Gaussian variable with covariance matrix $A$. This allows us to construct the desired family of finite dimensional distributions $\{X_j\}_{j \in  J}$, and then an application of Kolmogorov's extension theorem (link) concludes the proof. See \cite{lindgren2012stationary} page 72 for the details.
\end{proof}
There, is a nice characterization of the positive semi-definite functions first proved by Bochner in \cite{bochner1933monotone}
\begin{theorem}[Bochner's theorem]\label{Bochner's theorem}
    A function $r:\R^d\to \C$ is a continuous positive semi-definite function if and only if for some finite measure\\ $\mu:\Bb(\R^d)\to \R_+$
    \begin{equation*}
        r(u)=\int_{\R^d} e^{i\omega\cdot u}d\mu (\omega).
    \end{equation*}
    Furthermore, $\mu $ is unique.
\end{theorem}
We note that the implication part of the theorem can be seen from the fact
\begin{equation*}
    \sum_{j,k=1}^{n} a_j \overline{a}_k\int_{\R^d} e^{i\omega\cdot u_j}e^{-i\omega\cdot u_i} d\mu(w)=\int_{\R^d}\abs{\sum_{j=1}^{n} a_j e^{i\omega\cdot u_j}}^2d\mu(\omega)\geq 0.
\end{equation*}
The converse is however more technical and follows from a more general result on harmonic analysis of groups. We will discuss this in more detail later on.\\
\\
In the language of probability theory, another way of stating the above relationship is to say that $r$ is the  \emph{the characteristic function} of $\mu $ and to write $r=\hat{\mu}$. We note that $\mu $ is a \emph{Lebesgue-Stieltjes measure} (that is, a measure on $\mathcal{B}(\R^d)$ which is finite on bounded intervals) and as a result
the function $F:\R^d\to \R$ defined by
\begin{equation*}
    F(\omega):= \mu (-\infty,\omega].
\end{equation*}
Is, save additive constants, the unique \emph{distribution function} on $\R^d$ (that is,  $F$ is right continuous, non-decreasing and bounded) such that $\mu =dF$. See for example \cite{ash2000probability} page $30$ for a proof of this fact. We call respectively the measure $\mu$ and distribution function $F$ appearing in Bochner's theorem  the \emph{spectral measure} and \emph{spectral distribution} of $X$. In the case where $r$ is integrable, $\mu$ will have a density.
\begin{proposition}[{Fourier inversion}]\label{Fourier inversion}
    Let $r\in L^1(\R^d\to\C)$ be a continuous positive definite function with spectral distribution $dF$. Then
    $S(\omega):=\partial_1\cdots\partial_d F(\omega)$ exists almost everywhere, and we have
    \begin{equation*}
        S(\omega)=\qty(\frac{1}{2\pi})^d\int_{\R^d} {r}(u)e^{-i\omega\cdot u} d u;\quad    r(u)=\int_{\R^d}S(\omega)e^{i \omega\cdot u} d\omega.
    \end{equation*}
\end{proposition}
The proof is based on a classical result of probability called the inversion formula (see \cite{ash2000probability} page 292).\\
\\
In the case of Proposition \ref{Fourier inversion} above it is customary to say that $S$ is the \emph{spectral density} of $X$. We observe that the condition for the existence of the spectral density is quite weak. Essentially, all that is required is that the relationship between the field at points that are far decays to $0$ at a fast enough rate. For completeness, we mention the following equivalent definition which may be found in some references
\begin{lemma}
    A continuous function $r:\R^d\to\C$ is positive semi-definite if and only if for all $f\in L^1(\R^d\to\C)$
    \begin{equation*}
        \int_{\R^d}\int_{\R^d} r(x-y)f(x)\overline{f(y)}dx dy \geq 0.
    \end{equation*}
\end{lemma}
\begin{proof}
    Let us denote the above integral by $I(f)$. First, we show that $I(f)$ is well-defined and continuous as a function of $f \in L^1(\R^d\to\C)$. By Bochner's theorem (Theorem \ref{Bochner's theorem}) $r$ takes a maximum at  $0$ so
    \begin{equation*}
        \abs{I(f)}\leq r(0)\abs{\int_{\R^d}f(x)dx}\abs{\int_{\R^d}\overline{f(y)}dy}\leq r(0)\norm{f}_{L^1(\R^d\to \C)}^2
    \end{equation*}
    To prove the implication, one can first work with  $f\in C_c(\R^d\to\C)$. In this case, the above integral can be approximated by a sum over increasingly fine partitions
    \begin{equation*}
        I(f)=\lim_{N\to\infty}\sum_{j,k=1}^\infty\frac{1}{(x_{k+1}^{(N)}-x_k^{(N)})(x_{j+1}^{(N)}-x_j^{(N)})} r(x^{(N)}_k-x^{(N)}_j)f(x^{(N)}_k)\overline{f(x^{(N)}_j)}.
    \end{equation*}

    Since all these sums are positive, so must be the integral. Then we can use the density of $C_c(\R^d\to\C)$ in $L^1(\R^d\to\C)$ to get the general result. For the converse, given $a_1,..,a_n\in \C$, one can take $f_N\in\C_c(\R^d\to \C)$ converging to  $\sum_{j}a_j \delta(x_j)$ in the sense that
    \begin{equation*}
        \lim_{N\to\infty}\int_{\R^d}\int_{\R^d} r(x-y)f_N(x)\overline{f_N(y)}dx dy=
        \sum_{j,k=1}^{n} r(u_j-u_k)a_j\overline{a}_k.
    \end{equation*}
    Since, by hypothesis, the above integrals are greater than $0$ is their limit. This concludes the proof.
\end{proof}
The regularity of $r$ is intimately linked to the (mean square) continuity of $X$.
\begin{lemma}
    Let  $X:\R^d\to\C $ be a stationary field with covariance function $r$. Then if $r$ is continuous $X$ is mean square continuous. That is
    \begin{equation*}
        \lim_{h\to 0}\E[\abs{{X(u+h)-X(u)}}^2]=0,\quad\forall u\in\R^d.
    \end{equation*}
\end{lemma}
\begin{proof}
    We have that, by a calculation,
    \begin{multline*}
        \E[\abs{{X(u+h)-X(u)}}^2]=\E[\abs{X(h)}^2]+\E[\abs{X(h)}^2]-\E[X(u+h)\overline{X(u)}]\\-\E[X(u)\overline{X(u+h)}]=2(r(0)-\Re(r(h)).
    \end{multline*}
    If $r$ is continuous, since by definition $r(0)$ is positive we have that the above goes to $0$ as $h$ goes to $0$, which concludes the proof.
\end{proof}
The result has a converse (see \cite{lindgren2012stationary} page 34), but we shall not need it. Our future results will all be based on Bochner's theorem and thus require the continuity of $X$. In what remains of the work, we assume that all stationary fields have continuous covariance, or equivalently are mean square continuous.\\
\\
We next construct the spectral process $dZ$ of $X$. This, along with the spectral distribution, $dF$ form the most crucial objects in the spectral theory of stationary fields.
\begin{theorem}[The spectral theorem]\label{spectral theorem}
    If $X:\R^d\to\C$ is a stationary field with mean zero and spectral distribution $F(\omega)$, there exists a unique generalized field $dZ$ over $L^2(\R^d\to\C,dF)$ such that
    \begin{equation}\label{spectral representation}
        X(u)=dZ(e^{iu\cdot}),\quad\forall u\in\R^d
    \end{equation}
    Furthermore, $dZ$ defines a bijective isometry
    \begin{equation*}
        dZ:L^2(\R^d\to\C,dF)\to\overline{\text{span}(\{X(u)\}_{u\in\R^d})}\subset L^2(\Omega\to\C).
    \end{equation*}
    In particular, for any $f,g\in L^2(\R^d\to\C,dF)$,
    \begin{equation}\label{dZ properties}
        \E[dZ(f)]=0;\quad \E[dZ(f)\overline{dZ(g)}]=\int_{\R^d} f\overline{g}dF.
    \end{equation}
\end{theorem}
\begin{proof}
    We will first prove the existence of $Z$ and then use it to build $dZ$. Let us define given $u\in\R^d$
    \begin{equation}\label{constr phi}
        \varphi(X(u)):=e^{iu\cdot}.
    \end{equation}
    Where $e^{iu\cdot}$ is a function of $\cdot$ and is in $L^2(\R^d\to\C,dF)$ as $dF$ is a bounded measure. We then extend this definition linearly to the span of $\{X(u)\}_{u\in\R^d}$ by setting
    \begin{equation*}
        \varphi\qty(\sum_{j=1}^N\lambda_j X(u_j)):=\sum_{j=1}^N\lambda_j e^{iu_j\cdot}\quad\forall N\in \N,u_j\in\R^d.
    \end{equation*}
    Our next step is to show that $\varphi$ defines an isometry $$\varphi:\text{span}(\{X(u)\}_{u\in\R^d})\subset L^2(\Omega\to\C)\to L^2(\R^d\to\C,dF).$$
    Let $Y:=\sum_{j=1}^N\lambda_j X(u_j)$, then
    \begin{multline*}
        \norm{Y}_{L^2(\Omega\to\C)}^2=\norm{\sum_{j=1}^N\lambda_j X(u_j)}_{L^2(\Omega\to\C)}=\sum_{j,k=1}^N\lambda_j\overline{\lambda_k} \E[X(u_j)\overline{X(u_k)}]\\=\sum_{j,k=1}^N\lambda_j\overline{\lambda_k} r(u_j-u_k)
        =\sum_{j,k=1}^N\lambda_j\overline{\lambda_k} \int_{\R^d}e^{i\omega\cdot(u_j-u_k)}dF(w) \\= \int_{\R^d}\abs{\sum_{j=1}^N\lambda_je^{i\omega\cdot u_j}}^2dF(w)=\norm{\varphi(Y)}_{L^2(\R^d\to\C,dF)}^2
    \end{multline*}
    By the completeness of $L^2(\R^d\to\C, dF)$ and since $\varphi$ is an isometry, we can extend $\varphi$ to a mapping
    $$\varphi:\overline{\text{Span}(\{X(u)\}_{u\in\R^d})}\to L^2(\R^d\to\C,dF).$$ Furthermore, by the Stone-Weierstrass theorem, the span on $\{e^{iu\cdot}\}_{u\in\R^d}$ (which by construction is in the image of $\varphi$) is dense in $L^2(\R^d\to\C,dF)$. In consequence,  $\varphi$ is surjective (and thus bijective) and we can now define
    \begin{equation*}
        dZ(f):=\varphi^{-1}(f)\quad\forall f\in L^2(\R^d\to\C,dF).
    \end{equation*}
    Property \eqref{spectral representation} follows from the construction of $\varphi$ (see equation \eqref{constr phi}). The first part of \eqref{dZ properties} follows from the fact that $\varphi$ is valued in the closure of the span of $X(u)$ (which is of mean zero). The second part of \eqref{dZ properties} is because $dZ$ is an isometry, and thus also conserves the inner product. Finally, the uniqueness of $dZ$ follows from the fact that, by property \eqref{spectral representation}, $dZ$ is determined on a dense subset of $L^2(\R^d\to\C,dF)$ and the continuity of property \eqref{dZ properties}.
\end{proof}
In the literature (\cite{yaglom1987correlation} page 98, \cite{lindgren2012stationary} page 89) one may often find the notation
\begin{equation*}
    \int f(\omega)dZ(\omega):=dZ(f),\quad f\in L^2(\R^d\to\C,dF).
\end{equation*}
We will also use this notation, which corresponds with identifying $dZ$ with some kind of stochastic integration. To motivate this idea, let us set
\begin{equation}\label{Z def}
    Z(\omega):=dZ(1_{(-\infty,\omega]}),
\end{equation}
and consider for simplicity the $1$ dimensional case $d=1$. Then, given any simple function
$$f(\omega)=\sum_{j=1}^{n}  \lambda_{j} 1_{(\omega_j,\omega_{j+1}]}(\omega),$$
we have that by the linearity of $dZ$
\begin{equation*}
    dZ(f)=\sum_{j=1}^n\lambda_j dZ(1_{(\omega_j,\omega_{j+1}]})=\sum_{j=1}^n f(\omega_j) (Z(\omega_{j+1})-Z(\omega_j)).
\end{equation*}
Where the above sum is identical in form to that of Riemann-Lebesgue integration of $f$ against $dZ$. Furthermore, by the density of simple function in $L^2(\R,dF)$, given any $f\in L^2(\R,dF)$ we may take a sequence of simple functions $$f_n(\omega)=\sum_{j=1}^{n}  \lambda_{j} 1_{(\omega_j^{(n)},\omega^{(n)}_{j+1}]}(\omega),$$ converging to $f$. Then, since $dZ$ is an isometry (and thus continuous) we have that
\begin{equation}\label{approx stoch int}
    dZ(f)=\lim_{n\to\infty} dZ(f_n)=\lim_{n\to\infty}\sum_{j=1}^n f(\omega^{(n)}_j)\qty (Z(\omega^{(n)}_{j+1})-Z(\omega^{(n)}_j))
\end{equation}
So once more we recover $dZ(f)$ as a limit of stochastic integrals. This kind of construction mirrors the construction of the It√¥ integral. Where in our case $dZ$ takes the place of white noise $dW$. This also justifies why some authors call $dZ$ a \emph{$dF$ noise on $\R^d$} (\cite{adler2007random} page 105).\\
\\
\begin{observation}\label{obs 1}
    The spectral theorem means that in the future we will usually work with fields $X$ with mean $0$. However, many of the results can be generalized without difficulty to the case where $\E[X]\neq 0$ by noting that
    \begin{equation*}
        X(u)=\E[X]+\int_{\R^d}e^{i\omega\cdot u}dZ(\omega).
    \end{equation*}
    Where $dZ$ is now the spectral process of $X-\E[X]$. Equivalently, one can also write
    \begin{equation*}
        X=\int_{\R^d}e^{i\omega\cdot u}\widetilde{dZ}(\omega).
    \end{equation*}
    Where now $\widetilde{dZ}=dZ+\E[X]\delta_0$ will no longer be a $dF$ noise but rather verify for any given $f\in L^2(\R^d\to\C)$.
    \begin{equation*}
        \E[\widetilde{dZ}(f)]=\E[X]f(0);\quad \E[(\widetilde{dZ}(f))^2]=\norm{f}_{L^2(\R^d\to\C)}+\E[X]^2f(0)
    \end{equation*}
\end{observation}
We now prove an interesting corollary of the spectral representation theorem.
\begin{corollary}\label{Gaussian integral}
    Let $X$ be a mean zero, a stationary field with  spectral distribution and spectral process $dF,dZ$ respectively. Then $\{X(u)\}_{u\in\R^d}$ is Gaussian if and only if.
    \begin{equation*}
        dZ(f)\sim\mathcal{N}\qty(0,\norm{f}^2_{L^2(\R^d\to\C,dF)})\quad\forall \in L^2(\R^d\to\C,dF).
    \end{equation*}
\end{corollary}
\begin{proof}
    We recall that, by the basic theory of Gaussian random variables, if $Y_n$ is a sequence of Gaussian random variables converging in distribution to $Y$ then $Y$ is also Gaussian. Furthermore, as we showed in Theorem \ref{spectral theorem} (the spectral theorem) for each $f$,
    $$dZ(f)\in\overline{\text{Span}(\{X(u)\}_{u\in\R^d})}\subset L^2(\R^d\to C,dF).$$
    Since convergence in $L^p$ implies convergence in distribution we deduce that if $X$ is Gaussian then $dZ(f)$ is also Gaussian. Reciprocally, if $dZ$ is Gaussian, then necessarily $X(u)=dZ(e^{i\omega\cdot u})$ is Gaussian. Finally, the calculation of the variance of $dZ$ follows once more using that $dZ$ is an isometry.
\end{proof}
In the conditions of the above corollary, we will say that $dZ$ is a \emph{Gaussian $dF$ noise on $\R^d$}. Additionally, we also note that $Z(\omega)$ defined as in \eqref{Z def} must be Gaussian as follows by setting $f=1_{(-\infty,\omega]}$. Next, we prove a stochastic version of Fubini's theorem, which we will use later in Subsection \ref{linear filters section}.
\begin{proposition}[Stochastic Fubini]\label{stochastic Fbini}
    Given $f\in L^1(\R^d\to L^2(\R^d\to\C,dF))$ and a stochastic $dF$ noise  $Z$, it holds that
    \begin{equation*}
        \int_{\R^d}\qty(\int_{\R^d}f(v,\omega)dv)dZ(\omega)=\int_{\R^d}\qty(\int_{\R^d}f(v,\omega)dZ(\omega))dv.
    \end{equation*}
\end{proposition}
\begin{proof}
    Firstly, a minimum requirement is that both sides be well-defined as functions in $L^2(\Omega\to\C)$. That is, that
    \begin{equation*}
        \int_{\R^d}f(v,\cdot)dv\in L^2(\R^d\to\C,dF);\quad   \int_{\R^d}f(\cdot,\omega)dZ(\omega)\in L^1(\R^d\to\C).
    \end{equation*}
    The leftmost inclusion holds by Minkowski's integral inequality as
    \begin{multline*}
        \norm{\int_{\R^d}f(v,\cdot)dv}_{L^2(\R^d\to\C,dF)}\leq\int_{\R^d}\norm{f(v,\cdot)}_{L^2(\R^d\to\C,dF)}dv\\=\norm{f}_{L^1(\R^d\to L^2(\R^d\to\C,dF))}<\infty
    \end{multline*}
    The right-hand side is also well-defined as, by Minkowski's inequality and the isometry of the stochastic integral
    \begin{multline}\label{stochastic fub}
        \norm{\norm{\int_{\R^d}f(\cdot,\omega)dZ(\omega)}_{L^1(\R^d\to\C)}}_{L^2(\Omega\to\C)}\leq \norm{\norm{\int_{\R^d}f(\cdot,\omega)dZ(\omega)}_{L^2(\Omega\to\C)}}_{L^1(\R^d\to\C)}\\=\norm{f}_{L^1(\R^d\to L^2(\R^d\to\C,dF))}<\infty.
    \end{multline}
    To now prove the equality consider the case where $f=1_{A\times B}$ is the indicator function of some rectangle where $A\in L^1(\R^d\to\C)$ and $B\in L^2(\R^d\to\C,dF)$. Then we have that
    \begin{align*}
        \int_{\R^d}\qty(\int_{\R^d}f(v,\omega)dv)dZ(\omega) & =\int_{\R^d}\lambda_d(A)1_B(\omega) dZ(\omega)=\lambda_d(A)dZ(1_B). \\
        \int_{\R^d}\qty(\int_{\R^d}f(v,\omega)dZ(\omega))dv & = \int_{\R^d}1_A(v)dZ(1_B)dv=\lambda_d(A)dZ(1_B).
    \end{align*}
    Where in the above we wrote $\lambda_d$ for the Lebesgue measure, used the linearity of the Lebesgue and stochastic integrals, and the definition of the stochastic integral. Thus, in this second case, the equality of the proposition holds. Moreover, if $f$ is a linear combination of such indicator functions, an identical reasoning holds. Finally, it is a standard argument to show that $f$ may be approximated by an increasing sequence of linear combinations of rectangles  $f_n\leq f$. By applying inequality \eqref{stochastic fub} together with the monotone convergence theorem we get
    \begin{align*}
        \norm{\int_{\R^d}\qty(\int_{\R^d}(f-f_n)(v,\omega)dv)dZ(\omega)}_{L^2(\Omega\to\C)} & \leq\norm{f-f_n}_{L^1(\R^d\to L^2(\R^d\to\C,dF))}\to 0 \\\norm{\int_{\R^d}\qty(\int_{\R^d}(f-f_n)(v,\omega)dZ(\omega))dv}_{L^2(\Omega\to\C)}&\leq\norm{f-f_n}_{L^1(\R^d\to L^2(\R^d\to\C,dF))}\to 0.
    \end{align*}
    Since the equality of the proposition holds for $f_n$ by the convergence above we conclude the same for  $f$ as desired
\end{proof}
\section{The multidimensional and $H$ valued case}
The results of the previous subsection for complex-valued fields extend naturally to the multidimensional case, where $X$ is valued in $\C^n$. To the best of our knowledge the case where $X$ is valued in some (possibly non-separable) Hilbert space $H$ is not treated in the literature, however, this case follows without too much difficulty from the multidimensional case. \\
\\
We begin by extending the definition of  stationarity.

\begin{definition}
    Let $H$ be a complex Hilbert space with orthonormal basis $\{e_j\}_{j\in J}$. Then we say that a a field $X$  indexed over $G$ and with values in $H$ is stationary if it is square-integrable and for all $u,v \in G$ and $j, k\in J$ there are functions $r_{jk}: G\to H$ verifying
    \begin{equation*}
        \E[X(u)]=\E[X(v)];\quad r_{jk}(u-v)=\E[{X_{j}(u)\overline{X_k}(v)}]-\E[{X_{j}(u)]\overline\E[{X_k}(v)}].
    \end{equation*}
    Where $X_j:=\br{X,e_j}$ is the projection onto element $e_j$.
\end{definition}
We will say that $r:\R^d\to\C^{J\times J}$ is the \emph{covariance function} of $X$ (with respect to $\{e_j\}_{j\in J}$). First, we show that, as is desirable, stationarity is independent of the basis chosen.
\begin{lemma}
    Let $X$ be a stationary field valued in a Hilbert space $H$ with orthonormal basis $\{e_j\}_{j\in J}$. Then $X$ is also stationary with respect to any other orthonormal basis $\{v_\beta\}_{\beta\in K}$.
\end{lemma}
\begin{proof}
    We must check that the covariance of the projections onto $v_\beta$ only depends on the distance. This follows from the fact that, by Plancherel's theorem
    \begin{multline*}
        \E[\br{X(u),v_j}\overline{\br{X(v),v_k}}]=\E\qty[\sum_{l \in  J}X_l\br{e_l,v_j}\overline{\sum_{m\in J}X_m \br{e_m,v_k} }]\\= \sum_{l \in  J}\sum_{m \in  J} \br{e_l,v_j} \overline{\br{e_m,v_k}}r_{l m}(u-v)=:\tilde{r}_{ j k }(u-v) .
    \end{multline*}
    Where the commutation of the sum is justified  as, once more by Plancherel, the sums on the right-hand side of the first equality  converge in the space $L^2(\Omega\to\C)$.
\end{proof}
Though stationarity is independent of the basis chosen, the covariance function won't be. In the case where  $H=\C^n$, we will always consider the covariance function with respect to the canonical basis on $\C^n$.
\\
\\
Having set up the theory for general Hilbert spaces, we now somewhat restrict to this case. That is, we consider a stationary field $\{X(u)\}_{u\in\R^d}$ valued in $\C^n$. The first step in extending the spectral theory of Section \ref{1d} is to extend Bochner's theorem. To do so one must be a bit careful as $r_{ j k }:=Cov(X_j,X_k)$ is \emph{not} necessarily positive semi-definite for $j\neq k$.
\begin{theorem}[Bochner's multidimensional theorem]\label{bochner 3d}
    If the covariance function of a square-integrable stationary field $\{X(u)\}_{u\in\R^d}$ valued in $\C^n$ is continuous, there exist complex measures $d\mu_{jk}$ such that
    \begin{equation*}
        r_{jk}=\int_{\R^d} e^{i\omega \cdot u} d\mu _{j k}(\omega),\quad \forall j,k\in{1,...,n}.
    \end{equation*}
\end{theorem}
\begin{proof}
    However, given any $a=(a_1,...a_n)\in\C^n$ the random process
    $Y:=a\cdot X$ is a stationary complex-valued field with covariance
    \begin{equation*}
        r_a:=\sum_{j,k=1}^n a_j\overline{a_k}r_{jk}.
    \end{equation*}
    Thus, by the $1$-dimensional Bochner's theorem (Theorem \ref{Bochner's theorem}) we can find some distribution function $dF_a$ such that
    \begin{equation}\label{a}
        r_a= \sum_{j,k=1}^{n} a_j \overline{a_k}r_{jk}=\int_{\R^d}e^{i\omega\cdot u}dF_a(\omega)
    \end{equation}
    Given $j, k\in\{1,..,n\}$ we define
    \begin{align*}
        a^{(j,k)} & :=(0,\cdots,0,\overset{(j)}{1},0,\cdots,0,\overset{(k)}{1},0,\cdots,0) \\
        b^{(j,k)} & :=(0,\cdots,0,\overset{(j)}{i},0,\cdots,0,\overset{(k)}{1},0,\cdots,0) \\
        c^{(j)}   & :=(0,\cdots,0,\overset{(j)}{1},0\cdots,0).
    \end{align*}
    From \eqref{a} we obtain that
    \begin{align*}
        r_{jk}+r_{kj}+r_{jj}+r_{kk}   & =\int_{\R^d}e^{i\omega\cdot u}dF_{a^{(jk)}}(\omega);\quad r_{jj} & =\int_{\R^d}e^{i\omega\cdot u}dF_{c^{(j)}}(\omega)  \\
        ir_{jk}-ir_{kj}+r_{jj}+r_{kk} & =\int_{\R^d}e^{i\omega\cdot u}dF_{b^{(jk)}}(\omega);\quad r_{kk} & =\int_{\R^d}e^{i\omega\cdot u}dF_{c^{(k)}}(\omega).
    \end{align*}
    The above system can be solved to obtain
    \begin{equation*}
        r_{jk}=\int_{\R^d}e^{i\omega\cdot u}dF_{jk}
    \end{equation*}
    Where $dF_{jk}$ is a complex linear combination of $dF_{a^{(jk)}},dF_{b^{(jk)}}, dF_{c^(j)},dF_{c^{(k)}}$ and is thus a \emph{complex} valued measure (link).
\end{proof}
From the above, we can quickly obtain Bochner's theorem for fields valued in Hilbert spaces
\begin{theorem}[Bochner's theorem for $H$ valued fields]\label{bochner hilbert}
    Let $H$ be a Hilbert space with orthonormal basis $\{e_j\}_{j\in J}$ and let $\{X(u)\}_{u\in\R^d}$ be a stationary $H$ valued field. Then, if the covariance function $r:\R^d\to \C^{J\times J}$ of $X$ with respect to $\{e_j\}_{j\in J}$  is continuous, there exist complex measures $\mu_{jk}$ such that
    \begin{equation*}
        r_{jk}=\int_{\R^d} e^{i\omega \cdot u} d\mu _{j k}(\omega),\quad \forall j,k\in{J}.
    \end{equation*}
    Furthermore, $\mu_{jj}$ are bounded real measures for all $j\in\{1,..,n\}$.
\end{theorem}
\begin{proof}
    Let us fix $j\neq k\in J$ and consider the $\C^2$ valued field $\{(X_j(u),X_k(u))\}_{u\in\R^d}$. Then by applying Theorem \ref{bochner 3d} (Bochner's multidimensional theorem) we obtain complex measures $\mu_{jk}$ verifying
    \begin{equation*}
        r_{jk}=\int_{\R^d} e^{i\omega \cdot u} d\mu _{j k}(\omega).
    \end{equation*}
    To  see that $\mu_{jj}$ are real-valued, it suffices to consider the complex-valued field $\{(X_j(u)\}_{u\in\R^d}$ and once more apply Bochner's theorem. This completes the proof.
\end{proof}
The analog of the spectral theorem (Theorem \ref{spectral theorem}) is also immediate
\begin{theorem}[Spectral theorem for $H$ valued fields]\label{spectral theorem hilbert}
    Let $H$ be a Hilbert space with orthonormal basis $\{e_j\}_{j\in J}$ and let $\{X(u)\}_{u\in\R^d}$ be a stationary $H$ valued field with mean zero and spectral measure $\mu_{jk}$. Then there exists a unique $H$ valued generalized field $dZ$ over $L^2(\R^d\to \C,dF)$ such that,
    \begin{equation}\label{spectral representation2}
        X(u)=dZ(e^{iu\cdot}),\quad\forall u\in\R^d
    \end{equation}
    Furthermore, $dZ$ defines a bijective isometry
    \begin{equation*}
        dZ:L^2(\R^d\to \C,\nu)\to\overline{\text{span}(\{X(u)\}_{u\in\R^d})}\subset L^2(\Omega\to H).
    \end{equation*}
    Where $\nu:=\sum_{j\in J} \mu_{jj}$.
    In particular, for any $f,g\in L^2(\R^d\to\C,\nu)$,
    \begin{equation}\label{dZ properties2}
        \E[dZ(f)]=0;\quad\E[\br{dZ(f),{dZ(g)}}]=\int_{\R^d} f\overline{g}\nu.
    \end{equation}
\end{theorem}
\begin{proof}
    Since $X_j:=\br{X,e_j}$ are stationary we deduce from the spectral theorem for $\C$ valued stationary fields (Theorem \ref{spectral theorem}) that there exist $dZ_j$ verifying the properties of said theorem for $X_j$. Let us set
    \begin{equation*}
        dZ(f):=\sum_{j\in J}dZ_j(f)e_j\quad\forall f\in L^2(\R^d\to\C).
    \end{equation*}
    Then, by the construction of $dZ_j$,
    \begin{equation*}
        X(u)=\sum_{j\in J}X_j(u) e_j=\sum_{j\in J}dZ_j(e^{iu\cdot})e_j=dZ(e^{iu\cdot}).
    \end{equation*}
    The fact that $dZ(u)\in \overline{\text{span}(\{X(u)\}_{u\in\R^d})}\subset L^2(\Omega\to H)$ also follows from the construction of $dZ$ and the properties of $dZ_j$. Finally, the isometry in \eqref{dZ properties2} uses the fact that $dZ_j$ define isometries and Plancherel as,
    \begin{equation*}
        \E[\br{dZ(f),{dZ(g)}}]=\sum_{j\in J}\E[dZ_j(f)\overline{dZ_j(g)}]=\sum_{j\in J}\int_{\R^d}f\overline{g} d\mu_{jj}=\int_{\R^d}f\overline{g} d\nu.
    \end{equation*}
    Where $\nu$ is well-defined since, as was observed at the end of Theorem \ref{bochner 3d}, $\mu_{jj}$ are real measures (as opposed to complex).
\end{proof}
We conclude this subsection by observing that the spectral theory may also be extended to cases where $\{X(u)\}_{u\in G}$ is a field over some group $(G,+)$. The following discussion is based on \cite{adler2007random} pages 119 and \cite{yaglom1961second}. \\
\\
First, we note that the extension of the definition of covariance function and stationarity has no complication. For simplicity, let us now assume that $G$ is a locally compact Abelian group and that $X$ is a stationary field valued in $\C$. We have that the group of characters
$$\hat{G}:=\{\text{ group homomorphisms } \omega:(G,+)\to(\C,\cdot): \abs{f(g)}=1,\quad\forall g\in G\}$$ can also be made into a locally compact Abelian group. Furthermore, one can show the existence of a spectral measure $\mu$ and a $\mu$ noise $dZ$ such that for all $g\in G$
\begin{equation*}
    r(g)=\int_{\hat{G}}\omega(g)d\mu(\omega);\quad X(g)=\int_{\hat{G}}\omega(g)dZ(\omega).
\end{equation*}
An example of this theory occurs for example when $G=\Z^d$ (that is, $X$ is a stationary sequence of random variables), then
$
    \hat{G}=\left\{e^{i\omega \cdot}:\omega\in[-\pi,\pi]^d\right\}
$
and we obtain that for each $k\in\Z^d$
\begin{equation*}
    r(k)=\int_{[-\pi,\pi]^d}e^{i\omega \cdot k}d\mu(\omega).
\end{equation*}
If on the other hand $G=-[\pi,\pi]^d$ then $
    \hat{G}=\left\{e^{ik \cdot}:k\in\Z^d\right\}
$ is discrete
and we get that for each $u\in-[\pi,\pi]^d $
\begin{equation*}
    r(u)=\sum_{k\in\Z^d}e^{ik \cdot u}\mu_k.
\end{equation*}

\section{SDES and stationary fields}
\subsection{Linear filters: Frequency and impulse response}
In this subsection, we consider a very useful linear transformation of stationary fields.
\begin{definition}\label{frequency response def}
    Given a mean zero stationary field $X:\R^d\to\C$ with spectral measure and process  $dF,dZ$ respectively, a \emph{linear invariant filter} is an operator $\Ff_g$ of the form
    \begin{equation*}
        \Ff_g(X)(u):= \int_{\R^d} g(\omega)e^{i\omega\cdot u} dZ(\omega).
    \end{equation*}
    where $g \in L^2(\R^d\to\C,dF)$ is called the \emph{frequency response} (or \emph{transfer function}).
\end{definition}
By the equality
\begin{equation*}
    X(u)=\int_{\R^d}e^{i\omega\cdot u}dZ(\omega)
\end{equation*}
it is natural to view $dZ$ as the Fourier transform of $X$. This is quite similar to the concept of Fourier multipliers appearing in Harmonic analysis. That is, we are modifying our \emph{input field} $X$ by multiplying its Fourier transform by some function $g$. Thus modifying the frequencies that appear in its representation. Formally, if we write  $\Ff$ for the Fourier transform we have that
\begin{equation}\label{formal}
    \Ff_g(X)=\Ff^{-1}(g\Ff(X))
\end{equation}
A calculation shows that the following two properties are verified
\begin{lemma}
    Let $\Ff_g$ be a linear filter, and consider two stationary fields $X, Y$ such that $X+Y$ is also stationary.
    \begin{enumerate}
        \item Linearity: $\Ff_g(X+Y)=\Ff_g(X)+\Ff_g(Y)$.
        \item Translation invariance: Given any $v\in \R^d$ we have that  $\tau_v\Ff_g= \Ff_g\tau_v$ where $\tau_v:\R^d\to \R^d$ defined by $\tau_v(u)=u+v$ is the translation by $v$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    The linearity follows from the fact that, if $X, Y$ have respective spectral processes  $Z_1, Z_2$, then  $X+Y$ has spectral process  $Z_1+Z_2$. The spectral invariance follows from the fact that the spectral representation of  the translation $\tau_v X:= X(\cdot +v)$ is, just like in Fourier analysis $e^{i\omega v}Z_1$.
\end{proof}
One can also generalize the definition of a linear filter as any operator that satisfies the two properties above. Though we will not need to do so ourselves. One way of defining a linear translation invariant operator is by setting for $h\in L^1(\R^d\to\C)$
\begin{equation}\label{impulse response}
    Y(u):=\int_{\R^d} h(v)X(u-v) dv.
\end{equation}
We note that the integral in\eqref{impulse response} is well-defined as, by Fubini's theorem,
\begin{equation*}
    \mathbb{E}[Y]=\int_{\R^d} h(v)\mathbb{E}[X(u-v)] dv=\mu\int_{\R^d} h(v)\leq \mu\norm{h}_{L^1(\R^d\to\C)}.
\end{equation*}
Where $\mu$ is the constant mean of the field.
The terms appearing in \eqref{impulse response} respectively: the \emph{response function} $Y$,  the \emph{input function} $X$,and  the \emph{impulse response} $h$. This last name is motivated by the fact that if $X(u)$ is an impulse $\delta _0(u)$ (yes we know this isn't a stationary field) then $Y(u)=h(u)$.\\
\\
A crucial fact is that the impulse response $h$ and the frequency response $g$ have an intimate relationship. The frequency response corresponding to  $h$ is $(2\pi)^d\hat{h}$, whereas the impulse response corresponding to $g$ is $(2\pi)^{-d}\check{g}$. That is,
\begin{equation*}
    g=(2\pi)^d\hat{h};\quad h=\frac{1}{(2\pi)^d}\check{g}.
\end{equation*}
We encode this in the result below.
\begin{proposition}
    Given a  stationary field $X:\R^d\to\C$ with mean zero and\\ $h\in L^1(\R^d\to\C)$  it holds that
    \begin{equation*}
        \mathcal{F}_{\hat{h}}(X)(u)=\frac{1}{(2\pi)^d}\int_{\R^d} h(v)X(u-v) dv.
    \end{equation*}
    As a consequence, if  $g,\check{g}\in L^1(\R^d\to\C)$ then
    \begin{equation*}
        \mathcal{F}_g(X)(u)=\frac{1}{(2\pi)^d}\int_{\R^d} \check{g}(v)X(u-v) dv.
    \end{equation*}
\end{proposition}
\begin{proof}
    We have that, by definition of impulse response and the spectral representation of $X$,
    \begin{multline*}
        \int_{\R^d} h(v)X(u-v) dv= \int_{\R^d} \qty(h(v)\int_{\R^d} e^{i\omega (u-v)}dZ(\omega)) dv\\=\int_{\R^d}\qty(\int_{\R^d} h(v)e^{-i\omega v}dv) e^{i\omega\cdot u}dZ(\omega)=(2\pi)^d\Ff_{\hat{h}}(X).
    \end{multline*}
    Where in the above we used the stochastic Fubini theorem (Proposition \ref{stochastic Fbini}). The second part of the proposition follows from Fourier inversion. We have that $\hat{\check{g}}=g$ so
    \begin{equation*}
        \mathcal{F}_g=\mathcal{F}_{\hat{\check{g}}}=\frac{1}{(2\pi)^d}\int_{\R^d} \check{g}(v)X(u-v) dv.
    \end{equation*}
\end{proof}
The fact that the constant $(2\pi)^d$ pops up is a consequence of the exponent that appears in our definition of the Fourier transform. If one takes the exponential of $-2\pi i\omega x$ instead no constant appears here, however it would appear when we later take derivatives of stationary processes.\\
\\
In conclusion, the frequency and impulse response function $g,h$ form a Fourier transom pair, and we can thus move between one and the other when sufficient integrability conditions are met. In a way, this makes sense, as $g$ acts on the frequency domain while the $h$ acts on the original domain. Furthermore, heuristically, we have that with the purely formal notation in \eqref{formal}
\begin{equation*}
    \Ff_g(X)=\Ff^{-1}(g\Ff(X))=(2\pi)^{-d}\Ff^{-1}(g)*X=(2\pi)^{-d}h*X.
\end{equation*}
Where it was used that the (inverse) Fourier transform of the product is the convolution of the (inverse) Fourier transforms.
This said,
in a way,  we still have not answered the most pressing question. Is the result of applying a linear filter to a stationary field another stationary field?

\begin{proposition}\label{dens linear filter}
    Given the output functions $Y_1:=\Ff_{g_1}(X),Y_2:=\Ff_{g_2}(x)$ of a linear filtering, we have that their mixed covariance function is
    \begin{equation*}
        Cov(Y_1(u),Y_2(v))=\int_{\R^d}e^{i\omega (u-v)}g_1(\omega)\overline{g_2}(\omega) dF(\omega)    .
    \end{equation*}
    As a result, the output $Y=\Ff_{g}(X)$ of a linear filter is a stationary field. Furthermore, if $X$ has spectral density  $S_X(\omega)$ then $Y$ has spectral density
    $$S_Y(\omega)=|g(\omega)|^2 S_X(\omega).$$
\end{proposition}
\begin{proof}
    The first part of the proof is a direct application of the isometry for the stochastic integral against $dZ(w)$ and the definition of linear filter. Both the stationarity of  $Y=\Ff_g(X)$ and the calculation of its spectral density follow quickly by setting $g_1=g_2=g$.
\end{proof}
\subsection{Differentiation as a linear filter}
We now give a couple of examples of linear filters. First, consider $g=1_A$ where  $A$ is some measurable subset of  $\R^d$. Then $\Ff_g$ corresponds to ``filtering'' out the frequencies of $X$ outside  $A$. Another example of paramount importance occurs is that, under some integrability conditions, differentiation is itself a linear filter. First, we define in what sense we take derivatives of random fields.
\begin{definition}
    Given a $\C^n$ valued random field $\{X(u)\}_{u\in\R^d}$, we say that \emph{$X$ is mean square differentiable with respect to $u_i$ at $u$} if the following limit exists in $L^2(\Omega\to\C^n)$
    \begin{equation}\label{1 diff}
        \partial_j X(u)=\lim_{h \to 0}\frac{X(u+he_j)-X(u)}{h}\in L^2(\Omega\to\C)
    \end{equation}
    Where $e_j=(0,\ldots,\overset{(i)}{1},\ldots 0)$ is the $i$-th basis vector in  $\R^d$.
    Given some $m\in \N$ and $\beta\in\{1,\ldots,d\}^m$, if the iterated limits below exist, we write
    \begin{equation*}
        \frac{\partial X}{\partial u^\beta}(u):=\partial _{\beta_1}\ldots \partial _{\beta_d}X(u)\in L^2(\Omega\to\C) .
    \end{equation*}
    Now given $\alpha=(\alpha_1,\ldots,\alpha_d)\in \N^d$    we say that \emph{$X$ is  $\alpha$ times mean square differentiable at $u$} if for all permutations $\sigma$ of $\{1,\ldots,\sum_{n=1}^{d}\alpha_i\}$ it holds that
    \begin{equation}\label{alpha diff}
        D^\alpha X(u):=\frac{\partial X}{\partial u^\beta}(u)= \frac{\partial X}{\partial u^{\beta_\sigma}}(u);\quad \beta:=(1,\ldots,\overset{(\alpha_1)}{1},2\ldots \overset{(\alpha_2)}{2},\ldots,\overset{(\alpha_d)}{d})
    \end{equation}
    Finally, we say that \emph{$X$ is mean square differentiable with respect to $u_i$} and that \emph{ $X$ is $\alpha$ times mean square differentiable} if respectively \eqref{1 diff},\eqref{alpha diff}  hold for all $u\in \R^d$.
\end{definition}
Admittedly, the notation in the above definition is a bit messy. In words, we say that $X$ is (mean square differentiable) with respect to  $u_i$ if the usual limit exists in  $L^2(\Omega\to\C)$ and that the $\alpha$-th derivative exists if so do all the other derivatives of lower order and the order of differentiation does not matter.\\
\\
What does all this have to do with linear filters? Well, as in calculus, we have that mean square differentiation is linear and commutes with translation. As a result, we may suspect that differentiation is given by some linear filter $\Ff_g$. Under some integrability assumptions, this is exactly what occurs, as we show in the next proposition
\begin{theorem}[Differentiation of fields]\label{diff fields theorem}
    Given a mean $0$ stationary  field $X:\R^d\to\C$ with spectral distribution $dF$ the following are equivalent
    \begin{enumerate}
        \item $X$ is $\alpha$ times mean square differentiable.
        \item $r$ is $2\alpha$ times square differentiable in some interval of $0$.
        \item $r$ is $2\alpha$ times square differentiable and $D^\alpha X$ has covariance function
              \begin{equation*}
                  r_{D^\alpha X}=(-1)^{\abs{\alpha}}D^{2\alpha}r.
              \end{equation*}
        \item $\omega^\alpha:=\omega^{\alpha_1}\cdots \omega_d^{\alpha_d}\in L^2(\R^d\to\C,dF)$.
        \item $X$ is $\alpha$ times mean square differentiable with
              \begin{equation}\label{deriv X equation}
                  D^\alpha X(u)=\int_{\R^d}(i\omega)^{\alpha}e^{i\omega\cdot u}dZ(\omega).
              \end{equation}
    \end{enumerate}
\end{theorem}
\begin{proof}
    To prove the equivalence between the points, it is sufficient to prove it when $\abs{\alpha}=1$ and then apply an iteration. Let us take then\\ $\alpha=(0,\ldots,\overset{(i)}{1},\ldots 0)$. We have that, by the Loeve condition (link) for $L^2$ convergence, the derivative exists if and only if the limit
    \begin{multline*}
        \lim_{h,k \to 0}\E\qty(\frac{X(u+he_j)-X(u)}{h}\cdot\overline{\frac{X(u+ke_j)-X(u)}{k}})\\=
        \lim_{h,k \to 0}\frac{r((h-k)e_j)-r(he_j)-r(-ke_j)+r(0)}{kh}
    \end{multline*}
    exists as an element of $L^2(\Omega\to\C)$. Furthermore, if $r$ is twice differentiable in a neighborhood, we recognize the fraction in the above limit as an approximation to $-\partial_j^2r(0)$. This shows that $2$ implies  $1$ (for more details see \cite{lindgren2012stationary} page 35.) We now show that $1$ and $4$ are equivalent. To see that $1$ implies  $4$ we can use that
    \begin{multline*}
        -\partial_j^2 r(0)=\lim_{h \to 0}\mathbb{E}\qty[\abs{\frac{X(u+he_j)-X(u)}{h}}^2]=\lim_{h \to 0}2\frac{r(0)-\Re(r(he_j))}{h^2}\\=\lim_{h\to 0}
        \int_{R^d}2\frac{1-cos(\omega_j h)}{(\omega_jh)^2}\omega_j^2dF(\omega)\geq\int_{\R^d}\omega_j^2dF(\omega).
    \end{multline*}
    Where the first equality was already shown to be implied by point $1$ (link) and the inequality is due to Fatou's lemma. The fact that $4$ implies $2$, and thus also $1$, is proved similarly by the dominated convergence theorem.  We now prove the equivalence of $4$ and $5$. Suppose that  $4$ holds. Then we have  that, by the dominated convergence theorem
    \begin{equation*}
        i\omega_j=\lim_{h\to 0}\frac{e^{i\omega_j h}-1}{h}\in L^2(\R^d\to\C,dF).
    \end{equation*}
    By the isometry verified by $dZ$  and the spectral representation in \eqref{spectral representation} (the spectral representation of $X$)   we deduce that
    \begin{multline*}
        \partial_j X(u)=\lim_{h \to 0}\frac{X(u+he_j)-X(u)}{h}=\lim_{h \to 0}\int_{\R^d}e^{i\omega\cdot u} \frac{e^{i\omega_j h}-1}{h} dZ(\omega)\\
        =\int_{\R^d}i\omega_j e^{i\omega\cdot u}dZ(\omega)\in L^2(\Omega\to\C).
    \end{multline*}
    On the other hand, we have that $$i\omega_je^{i\omega\cdot u}\in L^2(\R^d\to\C,dF)\iff\omega_j\in L^2(\R^d\to\C,dF).$$
    Where the above are functions of $\omega$. This shows that $5$ implies $4$.
    \\
    \\
    Finally, trivially we have that $3$ implies $1$ so it only remains to show that all (or any) of the equivalent points $1,2,4,5$ implies $3$. This follows by a differentiation under the integral sign in
    \begin{equation*}
        r(u)=\int_{\R^d}e^{i\omega\cdot u}dF(\omega).
    \end{equation*}
    Which can be justified by point $4$.
\end{proof}
\begin{observation}
    It poses no difficulty to extend the above result to vector-valued stationary fields $X:\R^d\to\C^n$.
    We have that $X$ is $\alpha$ times differentiable
    if and only if its components are differentiable. Thus, for example, $X$ will be $\alpha$ times differentiable with derivative
    \begin{equation*}
        D^\alpha X_j(u)=\int_{\R^d}\omega^{\alpha}e^{i\omega\cdot u}dZ_j(\omega),
    \end{equation*}
    if and only if $r_{jj}$ are $2\abs{\alpha}$ times differentiable, if and only if $\omega^\alpha$ is in\\ $L^2(\R^d\to\C, dF_{jj})$. Where the above is for $j=1,...,n$.
\end{observation}
The just proved Theorem \ref{diff fields theorem} shows that, if the derivative of a stationary field exists, it is indeed given as the output of a linear filter $\Ff_g$ where in this case $g(\omega)=(i\omega)^\alpha$. In particular, we have that by the previous results on linear filters, the derivative of a stationary field is itself stationary. Furthermore, as a simple consequence, we obtain the following corollary,
which gives the spectral density of the derivative.
\begin{corollary}\label{density of derivative}
    Given a stationary field $X:\R^d\to\C$ with spectral density $S(\omega)$
    and $\alpha\in \N^d$ we have that $D^\alpha f$ exists if and only if $$S_\alpha(\omega):=\abs{\omega^{2\alpha}} f(\omega)\in L^1(\R^d\to\C).$$ In this case the spectral density of $D^\alpha f$ is $S_\alpha(\omega)$.
\end{corollary}
\subsection{Differential equations}\label{differential equations section}
Not only is there a link between linear filters and differentiation, but also between filters and solutions to ODEs, SDEs, and even SPDEs. The basic idea is that solutions to differential equations are often given by a convolution against an impulse response function $h$, which is determined by the structure of the equations.
\subsubsection{Ordinary differential equations}
Let us first consider the ordinary differential equation
\begin{align}\label{diff equation}
    a_0x^{(p)}(t)+a_1x^{(p-1)}(t)+\ldots+a_px(t) & =y(t) \\ x^{(j)}(0)&=x_{j,0},\quad j=1,...,p
\end{align}
Where $y:\R\to\R$ and $a_0,...,a_n\in\R$ are given whereas  $x:\R\to\R$ is unknown. Then, by a standard trick we consider higher order derivatives to be new variables by taking $$\mathbf{x}=(x,x',\ldots,y^{(p)});\quad \textbf{x}_0:=(x_{1,0},\ldots,x_{p,0}).$$  In this way we obtain that  equation \eqref{diff equation} is equivalent to the system of equations
\begin{equation*}
    \mathbf{x}'=A\mathbf{x}+\mathbf{y};\quad \mathbf{x}(0)=\textbf{x}_0.
\end{equation*}
Where
\begin{equation}\label{A def}
    A:=\left(\begin{array}{ccccc}
            0                  & 1                    & 0                    & \cdots & 0                  \\
            0                  & 0                    & 1                    & \cdots & 0                  \\
            \vdots             & \vdots               & \vdots               & \ddots & 1                  \\
            -\frac{a_{p}}{a_0} & -\frac{a_{p-1}}{a_0} & -\frac{a_{p-2}}{a_0} & \cdots & -\frac{a_{1}}{a_0}
        \end{array}\right);\quad \mathbf{y}(t):= \qty(\begin{array}{c}
            0      \\
            \vdots \\
            0      \\
            y(t)
        \end{array})
\end{equation}
By the basic theory of linear systems of ODEs (Duhamel's formula) this system of equations has as its solution
\begin{equation*}
    \textbf{x}(t)=e^{A t}\textbf{x}_0+\int_{0}^t e^{A(t-s)}y(t)ds.
\end{equation*}
If $x,y$ are random variables  $X,Y$, we just interpret that the equation must hold for almost every path and obtain analogously that for $\mathbf{Y}(t):=(0,\ldots,0,Y(t))$ and initial data $\mathbf{Y}(0)=(Y_{1,0},\ldots,Y_{p,0})$ the differential equation is solved by
\begin{equation*}
    \mathbf{X}(t)=e^{A t}\mathbf{Y}(0)+\int_{0}^t e^{A(t-s)}\mathbf{Y}(s)ds.
\end{equation*}
In general, the above solution is not stationary, as the integration is performed over the range $[0,t]$ as opposed to the whole of  $\R$. However, if all the eigenvalues of $A$ have negative real part, then the equation will be stable (that is, it will converge to $0$ exponentially fast when  $t$ goes to infinity) and we can consider as initial data $\mathbf{X}(0)=\int_{-\infty}^0 e^{-As}\mathbf{X}(s)ds$, which leads to a solution
\begin{equation*}
    \mathbf{X}(t)=\int_{-\infty}^t e^{A(t-s)}\mathbf{Y}(s)ds=\int_\R e^{A(t-s)}1_{[0,\infty)}(t-s)\mathbf{Y}(s)ds
\end{equation*}
Thus, we obtain a convolution of the form studied previously. Where now $\mathbf{h}(s)=e^{sA}1_{(0,\infty)}(s)$. This gives the following result:
\begin{proposition}[Stationary solutions to ODEs]
    Consider a mean zero stationary field $Y:\R\to\C$ and a family of linear constant coefficient ODEs
    \begin{equation*}
        a_0X^{(p)}(t)+a_1X^{(p-1)}(t)+\ldots+a_pX(t)=Y(t).
    \end{equation*}
    Suppose that the matrix $A$ in \eqref{A def}  has all negative eigenvalues. Then the above equation has a stationary solution, given by
    \begin{equation*}
        X(t)=\int_\R h(t-s){Y}(s)ds.
    \end{equation*}
    Furthermore, if the spectral density $S_X(\omega),S_Y(\omega)$ of $X,Y$ exist, then
    \begin{equation}\label{spectrd ODE}
        S_X(\omega)=\frac{S_Y(\omega)^2}{|P(i\omega)|^2}.
    \end{equation}
    Where $P(\omega):=a_0\omega^p+\ldots a_{1}\omega+a_p$.
\end{proposition}
\begin{proof}
    The proof is a consequence of the previous discussion and the fact that the solution to the family of ODEs is given by
    \begin{equation*}
        X(t)=\mathbf{X}_1(t)=\qty(\int_{-\infty}^t \mathbf{h}(t-s)\mathbf{Y}(s)ds)_1=\int_\R h(t-s)Y(s)ds.
    \end{equation*}
    Where $h(s)=\qty(e^{sA})_{1,p}1_{[0,\infty)}(s)$.\\
    \
    Finally, the formula \eqref{spectrd ODE} for the spectral density of $X$ follows from Corollary \ref{density of derivative}, where we calculated the spectrum of the derivative of a field.\\
\end{proof}
\subsubsection{Stochastic differential equations}
An analogous idea holds for SDEs, given an SDE of the form
\begin{align*}\label{diff equation}
    a_0X^{(p)}(t)+a_1X^{(p-1)}(t)+\ldots+a_pX(t)=dW(t)
\end{align*}
We will obtain a solution of the form
\begin{equation*}
    X(t)=\int_{-\infty}^t h(t-s)dW(s).
\end{equation*}
However, we note that the white noise $dW$ is not a stationary field in the sense previously defined. However, as we will see, a very useful interpretation is that $dW$ is a generalized stationary field with spectral density $1$ (different conventions in the definition of the Fourier transform to make it so different authors take the spectral density of white noise to be $1/2\pi$) and covariance function its ``Fourier transform'' $\delta_0(\omega)$.
\begin{definition}
    Let $dW$ be a White noise on $L^2(\R^d)$ and $h\in L^2(\R^d)$. Then we define the convolution of $h$ against $dW$ as $$h\star dW(u):=\int_{\R} h(u-v)dW(v).$$
\end{definition}
We can calculate the Fourier transform of the convolution as a multiplication of the Fourier transforms scaled by some constant.
\begin{lemma}
    Given $f,g\in L^2(\R^d\to\C^n)$, it holds that
    \begin{equation*}
        \widehat{f*g}=(2\pi)^d\hat{f}\hat{g}.
    \end{equation*}
\end{lemma}
By applying the definition of the Fourier transform in \eqref{ft def}, Fubini, and a change of variables we get that
\begin{proof}
    \begin{align*}
       (2\pi)^d \widehat{f*g}(\omega)&=\int_{\R^d} f*g(u)e^{-i\omega\cdot u}du= \int_{\R^d} \qty(\int_{\R^d}f(v)g(u-v)dv)e^{-i \omega\cdot u}du \\
        &=\int_{\R^d} f(v)e^{-i\omega v}\qty(\int_{\R^d} g(u-v)e^{-i\omega\cdot (u-v)}du)dv\\
        &=\int_{\R^d} f(v)e^{-i\omega v}\qty(\int_{\R^d} g(u)e^{-i\omega\cdot u}du)dv
        = (2\pi)^{2d}\hat{f}\hat{g}.
    \end{align*}
    As desired.
\end{proof}
We now prove that convolution against white noise defines a stationary process.
\begin{lemma}
    Let $dW$ be a White noise on $L^2(\R^d)$ and $h\in L^2(\R^d)$. Then $$X(u):=h\star dW(u)$$  is a  stationary field $X:\R^d\to\R$ with covariance function
    \begin{equation*}
        r(u)=\int_{\R^d} h(u-v)h(-v)dv.
    \end{equation*}
    Furthermore, if also $h\in L^1(\R^d)$ then $r\in L^1(\R^d)$ has spectral density
    \begin{equation*}        S=(2\pi)^d{\abs{\hat{h}}^2}.
    \end{equation*}
\end{lemma}


\begin{proof}
    We have the first equality holds by It√¥'s isometry, and we can write
    \begin{equation*}
        r(u)=\int_{\R^d} h(u-v)h(-v)dv=h* \tilde{h}(u).
    \end{equation*}
    Where $\tilde{h}(u):=h(-u)$. In consequence, by the preceding Lemma, if $h\in L^1(\R^d)$ we have that $r\in L^1(\R^d)$ and $X$ has spectral density
    \begin{equation*}    \hat{r}=(2\pi)^d\hat{h}\hat{\tilde{h}}=(2\pi)^d\hat{h}\overline{\hat{h}}=(2\pi)^d\abs{\hat{h}}^2.
    \end{equation*}
    This completes the proof.
\end{proof}
Using this lemma, we can show a similar result holds in the case of SDEs as for ODEs.
\begin{theorem}[Stationary solutions to SDEs]
    Consider an SDE     \begin{equation*}
        a_0X^{(p)}(t)+\ldots a_pX(t)=dW(t).
    \end{equation*}
    Where the matrix $A$ as defined in  \eqref{A def} has all negative eigenvalues. Then, there exists a stationary solution of the form $$X(t)=\int_{-\infty}^t h(t-s) dW(s).$$ Furthermore, if its spectral density $S(\omega)$ exists, then    \begin{equation*}
        S(\omega)={\abs{g(\omega)}^2}=\frac{2\pi}{|P(i\omega)|^2}.
    \end{equation*}
    Where $P(\omega):=a_0\omega^p+\ldots a_{1}\omega^{p-1}+a_p$.
\end{theorem}
\begin{proof}
    To show that there exists a solution of the given form, we can use the  theory for solutions to linear SDEs
    \begin{equation*}
        d \mathbf{X}=A \mathbf{X} +d \mathbf{W} .
    \end{equation*}
    We obtain the analog of Duhamel's formula for a linear system
    \begin{equation*}
        \textbf{X}(t)= e^{At} \textbf{X}_0+\int_{0}^te^{A(t-s)} d \textbf{W}(s).
    \end{equation*}
    For a proof based on It√¥'s formula see √òksendal's book on SDEs \cite{oksendal2003stochastic} page 65. To now obtain $X$ of the desired form, we set as in the theorem for ODEs
    $$X(t)= [\textbf{X}(t)]_{1} ;\quad\textbf{X}_0:=(0,\ldots,X_0);\quad X_0:=\int_{-\infty}^0e^{-As} dW(s).$$ Where $X_0$ is well-defined by the assumption that the SDE is stable. Finally, the resulting spectral density follows formally from Propositions \ref{dens linear filter} and \ref{density of derivative} together with the fact that the ``spectral density'' of one dimensional white noise is $2\pi$ (we'll see more of this later) (link). Alternatively, one can also compute $\abs{\hat{h}}^2$ where
    $h(s)=\qty(e^{sA})_{1,p}1_{(0,\infty)}(s)$.
\end{proof}
\subsubsection{Stochastic partial differential equations}
In fact the theory extends all the way to SPDES, we state a necessarily imprecise version of this generalization.
\begin{theorem}[Stationary solutions to SPDEs]\label{spde theorem}
    Consider an  SPDE of the form
    \begin{equation}\label{spde}
        \mathcal{L}X(u)=dW(u)
    \end{equation}
    Where $\mathcal{L}$ is a linear operator which does not depend on $u$. Let $P:\R^d\to\C$  solve
    \begin{equation*}
        \mathcal{L}e^{i\norm{\omega}^2}=P(\omega)e^{i\norm{\omega}^2}.
    \end{equation*}
    Then, if $A^{-1}\in L^2(\R^d)$. there exists a stationary solution of the form
    \begin{equation*}
        X(s)=h_{\mathcal{L}}*dW(s):=\int_{\R^d} h_{\mathcal{L}}(s-u)dW(u).
    \end{equation*}
    And $X$ has spectral density
    \begin{equation*}
        S={|\widehat{h_\mathcal{L}}|^2}=\frac{2\pi}{\abs{P}^2}.
    \end{equation*}
\end{theorem}
The theorem is shown in Section $3$ of Whittle's paper. \cite{whittle1963stochastic}. To show an example of the theorem in use, we have that if $\mathcal{L}$ is a finite sum of partial differential operators
\begin{equation*}
    \mathcal{L}=\sum_{\alpha\in B} \lambda_\alpha\left(\frac{\partial}{\partial u_1}\right)^{\alpha_1}\cdots \left(\frac{\partial}{\partial u_d}\right)^{\alpha_d},\quad \lambda_{\alpha}\in\C,\quad B\subset\N^d.
\end{equation*}
Then we have that
\begin{equation*}
    \mathcal{L}e^{i\norm{\omega}^2}=\qty(\sum_\alpha \lambda_\alpha(i\omega)^\alpha)e^{i\norm{\omega}^2}.
\end{equation*}
That is $P$ is a polynomial and by applying the previous theorem we obtain that the solution to $\Ll X=dW$ has spectrum
\begin{equation*}
    S=\frac{1}{|\sum_{\alpha\in B}\lambda_\alpha(i\omega)^\alpha|^2}.
\end{equation*}
Thus, we see that in this case, $P$ as in Theorem \ref{spde theorem} is a polynomial. We will call $P$ the polynomial associated to the operator $\mathcal{L}$.
\subsection{Simulations}\label{simulations section}
In this subsection, we look at how the previous theoretical results can be used to extract fields with some desired properties. We aim to calculate their spectral densities, covariance functions, and simulate from them.\\
\\
As was mentioned in the introduction, one of the fields most used in statics are Gaussian fields with Mat√©rn covariance function. These are a family of stationary Gaussian fields on $\R^d$ and that solve the SPDE given by the operator
\begin{equation}\label{matern SPDE}
    \mathcal{L}_M:=(-\Delta+\kappa^2)^{\alpha/2}.
\end{equation}
Where $\kappa>0,\alpha>d/2$ are two real parameters.
\begin{observation}[Meaning of the fractional derivative]
    Though we will not employ these details, we mention for completeness that the linear differential operator in \eqref{matern SPDE} is defined by its effect on the Fourier transform by
    \begin{equation*}
        \widehat{\mathcal{L}_Mf}(\omega):=(-\norm{\omega}^2+\kappa^2)^{\alpha/2}\Ff({f}),\quad f\in\mathcal{S}(\R^d).
    \end{equation*}
    Where, we used $\mathcal{F}$ to denote the Fourier transform and $\mathcal{S}(\R^d)$ is the Schwartz space on $\R^d$. That is, the space of infinitely differentiable functions whose derivatives decay infinitely fast. In fact, by a calculation , $\mathcal{L}_M$ can be shown to extend continuously to a continuous linear operator
    \begin{equation*}
        \mathcal{L}_M: H^{s+\alpha}(\R^d)\to H^s(\R^d).
    \end{equation*}
    Where $H^s(\R^d)$ is the Sobolev space of $s$-times differentiable functions
    \begin{equation*}
        H^s(\R^d):=\lbrace f\in L^2(\R^d):\norm{f}_{ H^s(\R^d)}:= (1+\norm{\omega}^2)^{s/2}\hat{f}(\omega)\in L^2(\R^d\to\C)\rbrace.
    \end{equation*}
    See for example \cite{lototsky2017stochastic} page $80$ for more details.
\end{observation}
Next, we wish to consider a stationary field whose spectral density is largest on a sphere of radius $r\in \R_+$. We can obtain such a field as a solution to an SPDE whose associated polynomial takes its minimum, we can consider for example the polynomial
\begin{equation*}
    P_S(\omega):=(\|\omega\|^2-r)^2+\lambda_1.
\end{equation*}
Which proceeds from the operator
\begin{equation}
    \Ll_S:=(-\Delta-r)^2+\lambda_1
\end{equation}
Where $\lambda_1\in\R\setminus\{0\}$ is fixed. Finally, we will also consider a stationary field whose spectrum has two peaks, one  at $v$ and another $-v$,  for some fixed $v\in\R^d$. To do so we can consider, for example,
\begin{equation*}
    P_T(\omega):=\norm{\omega-v}^2\norm{\omega+v}^2+\lambda_2.
\end{equation*}
Where $\lambda_2\in\R\setminus\{0\}$.
By Theorem \ref{spde theorem}, the operator associated is
\begin{equation}\label{two peaks}
    \mathcal{L}_T=(-\Delta+\|v\|^2)^2+4\nabla\cdot vv^T\nabla+\lambda_2
\end{equation}
Let us denote by $S_M,r_M,X_M$ the spectral density, the covariance function, and the stationary field that solve the linear equation given by $\mathcal{L}_M$. We also use analogous notation to define $S_S,r_S,X_S$ and $S_T,r_T,X_T$. We define $n=(n_1,...,n_d)\in\mathbb{N}^d, N=n_1\cdots n_d$ and for $x,y\in\R^d$
\begin{equation*}
    \frac{1}{x}:=\qty(\frac{1}{x_1},\ldots,\frac{1}{x_2});\quad xy=(x_1y_1,...,x_d,y_d).
\end{equation*}
We will also consider
\begin{equation}\label{omk}
    \omega_k:=\frac{2\pi k}{hn},\quad k\in \Ck_0:=\qty[\frac{n}{2},\frac{n}{2}-1]\cap \Z^d
\end{equation}
By Theorem \ref{spde theorem} and Bochner's theorem, we know that, for $i\in \{M,S,T\}$
\begin{equation*}
    S_i=\frac{2\pi}{P^2},\quad r_i=\hat{S_i}.
\end{equation*}
Thus, we need no numerical approximation to obtain $S_i$. To obtain, $r_i$ however, we need to approximate the Fourier integral over a grid. Doing so naively using Riemann sums has a cost $O(N^2)$. Thus, we will use a fast Fourier transform algorithm. This uses the discrete Fourier transform to reduce the order of complexity to $O(Nlog(N))$.\\
\\
Finally, we wish to simulate from the three fields $X_M,X_S,X_T$. To do so, we will use the spectral process of Theorem \ref{spectral theorem} (\red{Make sure your field has mean zero otherwise see Observation \ref{obs 1}}). Let us write $X$ for any one of these fields we have that, by this theorem
\begin{equation}\label{xsim}
    X(u)=\int_{\R^d}e^{i\omega\cdot u} dZ(\omega).
\end{equation}
Furthermore, since $X$ is a stochastic integral of a deterministic function against White noise,  $X$ is a Gaussian variable. In consequence, by Corollary \eqref{Gaussian integral}, we deduce that $dZ(f)$ is Gaussian for all  $f\in L^2(\R^d)$. By our observation in \eqref{approx stoch int}, we can approximate the stochastic integral in \eqref{xsim} above as
\begin{equation*}
    X(u)\sim \sum_{k\in \mathcal{K}_0} e^{i\omega_k\cdot u}Z_k.
\end{equation*}
Where $Z_k:=dZ((\omega_k,\omega_{k+1}])$ is a Gaussian random variable with variance
\begin{equation*}
    \norm{Z_k}^2_{L^2(\Omega\to\mathbb{C})}=\norm{Z_k}^2_{L^2(\R^d\to\C,dF)}=\int_{(w_k,w_{k+1}]}S(\omega)d\omega.
\end{equation*}
\red{This is what we talked about today, I don't know why Finn and I never used this though.}
Let us denote the variance of $dZ_k$ by  $\sigma _k$, so that,
\begin{equation*}
    dZ_k=\sigma _k(x_k+iy_k).
\end{equation*}
Where $x_k,y_k$ are iid unit, real-valued normal random variables for $k \in \Ck_+$.
\\
\red{By the isometry in \eqref{dZ properties}, we have that $Z_k,Z_j$ are independent for different  $k\neq j$. However, why are the real and imaginary part of $Z_k$  iid and had the same variance. Is this  a property or just something we assume? Why make such an assumption? Also I checked his spectral notes, and at the end of page three he says the complex and imaginary parts are not independent??}\\
Since we know that $X$ is real valued, its imaginary part must be zero, so as $n$ goes to infinity (in all directions)
\begin{multline}\label{sr+}
    X(u)\simeq\sum_{k\in \Ck_0} e^{i\omega_k\cdot u}Z_k=\sum_{k\in \Ck_0}\sigma _k(cos(\omega_k\cdot u) + i sin(\omega_k\cdot u))(x_k +iy_k)\\
    =\sum_{k\in \Ck_0}\sigma _k(x_{k}cos(\omega_k\cdot u)-y_{k}sin(\omega_k\cdot u))
\end{multline}
Using this together with the independence of $x_k,y_k\sim\mathcal{N}(0,1)$ gives that
\begin{multline}\label{eq1}
    r(u-v)=Cov(X(u),X(v))\\\simeq \sum_{k\in \Ck_0} \sigma_k^2(cos(\omega_k\cdot u)cos(\omega_k\cdot v)+sin(\omega_k\cdot u)sin(\omega_k\cdot v))\\
    =\sum_{k\in \Ck_0} \sigma _k^2 cos(\omega_k\cdot(u-v)).
\end{multline}
Where we used the formula for the cosine of the difference in the last equality.
By a change of variables and the period of the complex exponential, we have that if $u$ is divisible by  $h$ (i.e. $u=jh$ for some $j\in\mathbb{Z}^d$), then by Bochner's theorem and the periodicity of the complex exponential
\begin{multline*}
    r(u)=\sum_{k\in \Z^d} \int_{[-\frac{\pi+2\pi k}{h},\frac{\pi+2\pi k}{h})} e^{iu\cdot\omega}S(\omega)d\omega= \int_{[\frac{-\pi}{h},\frac{\pi}{h})} e^{iu\cdot\omega}\sum_{k\in \Z^d} S\qty(\omega+ \frac{ 2\pi k}{h})d\omega\\=\int_{[-\frac{\pi}{h},\frac{\pi}{h})} e^{iu\cdot\omega}\tilde{S}_h(\omega)d\omega.
\end{multline*}
Where we defined $\tilde{S}_h$ as the sum appearing in the above integral.
Since $r$ must be real, $S$ (and thus $\tilde{S}_h$) is real, we deduce that, if $u-v$ is divisible by  $h$, then
\begin{multline}\label{eq2}
    r(u-v)=\int_{[-\frac{\pi}{h},\frac{\pi}{h})} Re(e^{i(u-v)\cdot\omega})\tilde{S}_h(\omega)d\omega=\int_{[-\frac{\pi}{h},\frac{\pi}{h})}cos(\omega\cdot(u-v)) \tilde{S}_h(\omega)d\omega\\
    \simeq \sum_{k \in \Ck_0}\abs{\frac{2\pi}{hn}}\tilde{S}_h(\omega_k)cos(\omega_k\cdot(u-v)).
\end{multline}
Where in the last line we did an approximation to the integral over the interval $[-\pi/h,\pi/h)$ by subdividing it into $n/2$ parts.
Due to \eqref{eq1} and \eqref{eq2}, and by the definition of $\omega_k$ in \eqref{omk},  we set
\begin{equation*}
    \sigma _k^2:= \abs{\frac{2\pi}{hn}}\tilde{S}_h\qty(\frac{2\pi k}{hn}),\quad\forall k\in \mathcal{K}_0.
\end{equation*}
We can now calculate $X(s)$ according to the discrete approximation to the spectral representation in \eqref{sr+}. Thus, we are now able to calculate all of the terms we desire.
Now, I know what you're thinking, but what about generalized fields? Can they be stationary, what does the spectral theory say for them? Ok, maybe none of you were thinking that, in fact you may now be asking yourselves what a generalized field is. A good question indeed, and one that will be answered in the next post along with your other most burning questions related to Bochner's theorem and the spectral theorem for these fields (if you don't have these questions and want your money back, it's too late. Also, you didn't give me any money anyway, so you're not getting it back.).

\section{Appendix}
\subsection{The Bochner integral}
We now define integration for functions valued in a Banach space \begin{equation*}
    f:(\Omega,\mathcal{F},\mu)\to ((X,\norm{\cdot}),\mathcal{B}(X))
\end{equation*}
If we denote the class of simple functions by $\mathcal{A}$ we can define their integral as
\begin{equation*}
    \int f d\mu=\int \sum_{k=1}^n x_k 1_{A_k} d\mu=\sum_{k=1}^n x_k\mu({A_k})
\end{equation*}
If we take equivalence class and define the norm
\begin{equation*}
    \norm{f}_\mathcal{A}:=\int_\Omega \norm{f} d\mu.
\end{equation*}
Then we get that integration is a linear and absolutely continuous function
\begin{equation*}
    int: \qty(\mathcal{A},\norm{\cdot}_A)\to (X,\norm{\cdot}).
\end{equation*}
as for all $f\in\mathcal{A}$
\begin{equation*}
    \norm{\int_\Omega f}\leq\int_\Omega \norm{f} d\mu=\norm{f}_\mathcal{A}
\end{equation*}
This shows that we can extend integration in a unique way to the completion $\Bar{\mathcal{A}}$ of $\qty(\mathcal{A},\norm{\cdot}_A)$.
\begin{definition}
    We say a function $f:(\Omega,\mathcal{F})\to (X,\mathcal{B}(X))$ is strongly measurable if $f$ is measurable $f(\Omega)$ is separable.
\end{definition}
\begin{definition}
    For $1\leq p<\infty$ we define
    \begin{align*}
        \mathcal{L}^p(\Omega,\mathcal{F},\mu,X)       & =\left\{f:\Omega\to X: f \text{ is strongly measurable and } \int \norm{f}^p d\mu<\infty\right\}. \\
        \hat{\mathcal{L}}^p(\Omega,\mathcal{F},\mu,X) & =\left\{f:\Omega\to X: \int \norm{f}^p d\mu<\infty\right\}.
    \end{align*}
    We also define the semi-norms
    \begin{align*}
        \norm{f}_{L^p(\Omega\to X)}       & :=\qty(\int \norm{f}^p d\mu)^{1/p},\quad f\in\mathcal{L}^p(\Omega,\mathcal{F},\mu,X).       \\
        \norm{f}_{\hat{L}^p(\Omega\to X)} & :=\qty(\int \norm{f}^p d\mu)^{1/p},\quad f\in\hat{\mathcal{L}}^p(\Omega,\mathcal{F},\mu,X).
    \end{align*}
    Finally we take equivalence classes by the above semi-norms to obtain the metric spaces $L^p(\Omega\to X)$ and $\hat{L}^p(\Omega\to X).$
\end{definition}
A straightforward adaptation of the proof of the completion of $L^p$ spaces proves that $L^p(\Omega\to X)$ and $\hat{L}^p(\Omega\to X)$ are also complete.
\begin{proposition}
    Given a Banach space $X$, the space of $p$ integrable strongly measurable and measurable functions $L^p(\Omega\to X),\hat{L}^p(\Omega\to X)$ are Banach spaces.
\end{proposition}
\begin{proof}
    The proof is identical in both cases so we prove it only for $f\in L^p(\Omega\to X)$
    It suffices to show that if $f_n$ is such that
    \begin{equation*}
        \sum_{n=1}^\infty \norm{f_n}_{L^p(\Omega\to X)}<\infty.
    \end{equation*}
    Then there exists $f\in L^p(\Omega\to X)$ such that
    \begin{equation*}
        f=\sum_{n=1}^\infty f_n\in L^p(\Omega\to X).
    \end{equation*}
    To do so we apply Minkowski's inequality for real valued functions to show that
    \begin{equation*}
        \sum_{n=1}^\infty \norm{f_n}_X\in L^p(\Omega\to\R).
    \end{equation*}
    and is thus finite almost everywhere. Since $X$ is complete we have that the below sum converges pointwise almost everywhere to some function
    \begin{equation*}
        f(\omega):=\sum_{n=1}^\infty f_n(\omega)\in X.
    \end{equation*}
    Furthermore we have that $f$ is strongly measurable as it is the limit of strongly measurable functions and, by Fatou's lemma for real valued functions and the triangle inequality for norms
    \begin{multline*}
        \norm{f-\sum_{n=1}^N f_n}_{L^p(\Omega\to X)}=\norm{\sum_{n=N}^\infty f_n}_{L^p(\Omega\to X)}\leq \liminf_{M\to \infty}\norm{\sum_{n=N}^M f_n}_{L^p(\Omega\to X)}\\\leq \liminf_{M\to \infty}\sum_{n=N}^M \norm{f_n}_{L^p(\Omega\to X)}=\sum_{n=N}^\infty \norm{f_n}_{L^p(\Omega\to X)}\xrightarrow{N\to\infty} 0.
    \end{multline*}
    Which shows convergence in $L^p(\Omega\to X)$.
\end{proof}
In the standard construction of the Lebesgue integral it is used that every measurable function to $\mathbb{R}$ can be pointwise approximated by simple functions. One can achieve the same result for arbitrary metric spaces if the image of $f$ is separable.
\begin{lemma}
    Let $(E,d)$ be a metric space and let $f$ be strongly measurable. Then $f$ is the pointwise limit of simple functions $f_n$. Furthermore, \\$d_n(\omega):=d(f_n(\omega),f(\omega))$ is a non-increasing sequence for each $\omega\in\Omega$
\end{lemma}
\begin{proof}
    Consider $\{e_k\}_{k=1}^\infty$ to be dense in $f(\Omega)$. Now set
    \begin{equation*}
        f_n(\omega):= \argmin_{i\in\{1,...,n\}} \{d(f(\omega),e_i)\}
    \end{equation*}
    Since for each fixed $\omega$ the above distance goes to $0$ we have that
    \begin{equation*}
        \lim_{n\to\infty} f_n(\omega)=f(\omega),\quad \forall \omega\in\Omega.
    \end{equation*}
    Furthermore, $f_n$ is simple as $f_n(\Omega)\in \{e_1,...,e_n\}$ and the non-increasing property of $d_n(\omega)$ is clear by construction.
\end{proof}
In the above proof we see that the reason for requiring that the image of our class of integrable functions be separable is so that we can approximate them by simple functions.
\begin{proposition}
    Every function $f\in L^1(\Omega\to X)$ is the limit of simple functions.
\end{proposition}
\begin{proof}
    Since $f(\Omega)$ is dense we can apply the above lemma to obtain a sequence of simple functions $f_n$ converging point-wise to $f$. Furthermore we have that, by the monotone convergence theorem for integrals in $\mathbb{R}$
    \begin{equation*}
        \lim_{n\to\infty}\norm{f_n-f}_{L^1(\Omega\to X)}=\lim_{n\to\infty}\int_\Omega\norm{f_n-f}d\mu=0.
    \end{equation*}
\end{proof}
As a corollary we obtain the following
\begin{corollary}
    $\mathcal{A}$ is a dense subset of $\qty(L^1(\Omega\to X),\norm{\cdot}_{L^1(\Omega\to X)})$.
\end{corollary}
In consequence, by the linearity and continuity of the integral and the above density, we have a unique continuous extension of the integral to $L^1(\Omega\to X)$.
\begin{definition}
    We define the integral on $L^1(\Omega\to X)$ as the unique continuous extension with the norm $\norm{\cdot}_{L^1(\Omega\to X)}$ of the integral on $\mathcal{A}$. That is, given $f\in L^1(\Omega\to X)$ we define
    \begin{equation*}
        \int_\Omega f d\mu:=\lim_{n\to\infty} \int_\Omega f_n d\mu.
    \end{equation*}
    Where $f_n\in \mathcal{A}$ is any sequence such that $\norm{f-f_n}_{L^1(\Omega\to X)}\to 0$.
\end{definition}
As a result of the definition the following is immediate.
\begin{corollary}
    Let $f\in L^1(\Omega,X)$ with $X$ a Banach space, then
    \begin{enumerate}
        \item $\norm{\int_\Omega f d\mu}\leq\int_\Omega{\norm{f}d\mu} $
        \item Let $Y$ be another Banach space and $L\in L(X,Y)$. Then
              \begin{equation*}
                  \int_\Omega L\circ f d\mu=L\qty(\int_\Omega f d\mu).
              \end{equation*}
    \end{enumerate}
\end{corollary}
`'In this section we define some of the concepts that are needed to properly define an SPDE, we will consider SPDEs only in space. If one wished to consider time as well it would be necessary to construct the stochastic integral. This would require a substantial amount of functional and stochastic analysis. A construction mirroring the one done for SDEs can be found in Chapter 2 of \cite{liu2015stochastic}. An alternative approach based on expansions in an orthonormal basis can be found in \cite{lototsky2017stochastic} page 131.
\subsection{Reproducing kernel Hilbert spaces}
In this subsection we will consider all vector spaces to be complex.
\begin{definition}
    A reproducing kernel space is a Hilbert space $S$ of complex valued functions on some set $S$ (that is, $H\subset \C^S$ where $S$ is some set) and such that  point evaluation $\ell _s$, $$\ell _s(f):=f(s),\quad f\in H,$$ is continuous for all $s\in S$.
\end{definition}
For example the spaces $L^2(\R^d)$ do not fall into this category as pointwise evaluation is not even well defined. That is, unless we consider $S=L^2(\R^d)$ with pointwise evaluation defined as
\begin{equation*}
    f(s):=\br{f,s},\quad\forall s\in S.
\end{equation*}
In this case we would obtain that $L^2(\R^d)$ is a reproducing kernel space over itself with reproducing kernel $K(f,\cdot )=f.$ This construction can be used to view any Hilbert space as a reproducing kernel space over its dual. To give another example, by Sobolev embedding theory, $H^s(\R^d)$ are reproducing kernel spaces for $s>\frac{d}{2}$.
\begin{definition}
    A function $K:S\times S\to \C$ is said to be a \emph{positive-definite  Kernel} on $S$ if  $\{K(t_i,t_j)\}_{i,j=1}^n$ is a Hermitian, positive semi-definite matrix for all $t_i \in S,n \in \N$.
\end{definition}
For example the covariance function $K(t,s)=Cov(X(t),X(s))$ of a complex valued field is a positive semi-definite kernel.
\begin{proposition}\label{existence reproducing kernel}
    Given a reproducing kernel Hilbert space $H$ there exists a unique positive semi-definite kernel $K_H$ called the \emph{reproducing kernel} of $H$ such that
    \begin{equation*}
        f(s)=\br{f,K_H(s,\cdot )},\quad\forall f\in H.
    \end{equation*}
    Furthermore, given any positive-definite kernel $K$ their exists a reproducing kernel space $H_K$ such that $K$ is the reproducing kernel of $H_K$.
\end{proposition}
\begin{proof}
    We first prove the existence and uniqueness of $K_H$, by Riesz's representation theorem, for each $s\in S$ there exists $g_s\in H$ such that
    \begin{equation*}
        f(s)=\br{f,g_s}\quad\forall f\in H.
    \end{equation*}
    Let us define $K_H(t):=g_s(t)$, the defining property is verified by construction and it remains to see that $K_H$ is a positive definite
    kernel. This follows from the equality
    \begin{equation}\label{rep kernel}
        K_H(s,t)=\br{K_H(s,\cdot ),K_H(t,\cdot )}
    \end{equation}
    To prove the existence of $H_k$ we set
    \begin{equation*}
        \quad H_{K,0}:=span(\{K(s,\cdot ):s\in S\});\quad \br{\sum_{j=0}^{n}K(s_j,\cdot ),\sum_{k=0}^{m} K(t_k,\cdot )}_{H_{K,0}}:=\sum_{j=0}^{n}\sum_{k=0}^{m}K(s_j,t_k).
    \end{equation*}
    We note that the above defines a inner product by the symmetry and positivity condition on $K$ and that pointwise evaluation is continuous by construction and Cauchy Schwartz. All that is left is to take the completion  $H_K:=\overline{H_{k,0}}$ as it is a verification that $K$ is the reproducing kernel of  $K_H$.
\end{proof}
The equality \eqref{rep kernel} motivates the name reproducing kernel. Another equality of note, which can be easily derived from the above is that, given an orthonormal basis $e_k$ of $H$
\begin{equation*}
    K_H(s,t)=\sum_{n=0}^{\infty} e_n(s)\overline{e_n(t)}.
\end{equation*}

Given any topological vector space $(V,\Bb(V),\mu )$ whose measure allows for the existence of second moments, we can construct a positive definite kernel on $V'$ as
\begin{equation*}
    C_\mu (\ell ,h):=\int_{V}\ell (v) \overline{h(v)}\mu (dv)-\int_{V}\ell (v) \mu (dv)\overline{\int_{V}h(v)\mu (dv)}.
\end{equation*}
We call this the \emph{covariance kernel} associated with $\mu $. By setting $S=V'$ we may apply the previous Proposition \ref{existence reproducing kernel}
to deduce that there exists a Hilbert space $H_\mu $ whose reproducing kernel is $C_\mu $. We call this space the \emph{reproducing kernel Hilbert space of $\mu $}. If $\mu $ is the distribution of some $V$ valued random variable $X$ then we also say that $H_\mu $ is the  \emph{reproducing kernel Hilbert space of $X$}.\\
\\
Alternatively, we can also view  $C_\mu $ as a function from $V'\to \tilde{V'}'$. Where given a vector space $E$ we denote the space of antilinear maps by   $\tilde{E}'$ In the case where $V$ is a Hilbert space $H$ we can identify $H$ with both $H'$ and $\tilde{H'}'$. Through these identifications we deduce for each $h\in H$ the existence of  some $h_\mu \in H$ such that
$C_\mu (h,\cdot )=\br{h_\mu,\cdot  }$. That is
\begin{equation*}
    C_\mu (h,x)=\br{h_\mu,x},\quad\forall h,x \in H .
\end{equation*}
By another abuse of notation, using the previous identifications we can consider
\begin{align*}
    C_\mu : H & \longrightarrow H             \\
    h         & \longmapsto C_\mu (h) = h_\mu
    .\end{align*}
That is, to gather all the notation together
\begin{equation*}
    C_\mu (\br{\cdot ,h},\br{\cdot ,x})=C_\mu (h,x)=\br{C_\mu (h),x}\quad\forall h,x\in H.
\end{equation*}



\subsection{Gaussian Random variables}
\begin{definition}
    Given a topological vector space $V$, we say that a \emph{Gaussian measure } is a measure on $\Bb(V)$ such that $\ell $ is a real valued Gaussian variable for all $\ell \in E^*$.
\end{definition}
\begin{definition}
    A $V$ valued random variable is said to be Gaussian if $\mathbb{P}_X$ is a Gaussian measure on $V$.
\end{definition}
We note that in the case $V=\R^d$ this definition coincides with the classic one.
\begin{proposition}\label{covarianceariance kernel gaussian} Let $H$ be a separable Hilbert space and identify  $H$ with $H'$, then:
    \begin{enumerate}
        \item Given a Gaussian measure $\mu $ on  $(H,\Bb(H))$, its covariance operator is trace class. That is, $C_\mu \in \Ll_1^+(H)$.
        \item Given $K\in \Ll_1^+(H)$ there exists a Gaussian measure $\mu $ on $(H,\Bb(H))$ such that $C_\mu(u,v)=\br{Ku ,v}$.
    \end{enumerate}
\end{proposition}
\begin{proof} We prove each point in order

    \begin{enumerate}        \item Consider an orthonormal basis $e_n$ of $H$ (which we recall we are identifying with $H'$), then we have that
              \begin{equation*}
                  \sum_{n=0}^{\infty} \br{C_\mu (e_n),e_n}_H=\sum_{n=0}^{\infty} C_\mu (\br{\cdot ,e_n},\br{\cdot ,e_n})_H=\int_{H}\norm{v}^2 \mu (dv)<\infty.
              \end{equation*}
              Where the last equality is due to Fernique's theorem,which says that Gaussian measure have exponential decrease.
        \item By theorem .... we can diagonalize $K$ to obtain a orthonormal basis  $e_n$ with  $Ke_n=\lambda e_n$. By theorem we can build a probability measure $\mathbb{P}$ on $(H,\Bb(H))$ together with a sequence $G_n$ of iid unit Gaussian vectors. Let us set
              \begin{equation*}
                  \mu :=\mathbb{P}_X;\quad X=\sum_{n=0}^{\infty} \sqrt{\lambda _n} G_ne_n .
              \end{equation*}
              Then we have that $\mu $ is a Gaussian measure and, by the  independence and normality of $G_n$
              \begin{multline*}
                  C_\mu (u,v)=\int_{H}\br{u,h}\overline{\br{v,h}} d\mathbb{P}_X=\sum_{n,m=0}^{\infty}\sqrt{\lambda_n}\sqrt{\lambda_m}  \br{u,e_n}\overline{\br{v,e_m}}\E[G_nG_m]\\
                  =\sum_{n=0}^{\infty}\lambda_n\br{u,e_n}\overline{\br{v,e_n}}=\br{Ku,v}.
              \end{multline*}

    \end{enumerate}
\end{proof}
We now have a relationship between operators in $\Ll_1^+(H)$ and Gaussian measures. A natural question is whether this relationship is bijective. To do so we introduce the Fourier transform. For Gaussian measures on separable Banach spaces we have the following
\begin{proposition}
    Let $E$ be a separable Banach space, then $\mu $ is a Gaussian measure on $E$ iff for every linear function  $\ell :V\to\R$
    \begin{equation*}
        \hat{\mu }(\ell ):=\int_{E} e^{i\ell (x)}d\mu(x)=e^{i\ell(m)-\frac{1}{2}C_\mu (\ell ,\ell )}.
    \end{equation*}
    Where
    $        m:=\int_{E}x \mu (dx).
    $
\end{proposition}
\begin{proof}
    The implication is an application of theory of $1$-dimensional Gaussian random variables. By definition we have that $\mu $  is a Gaussian measure if and only if all real valued continuous linear functions $\ell:E\to \R$ are Gaussian random variables. Which in turn implies that their characteristic function $\varphi_\ell $ verifies that
    \begin{equation*}
        \hat{\mu }(\ell )=\int_{E} e^{i\ell (x)} \mu (dx)=\varphi_{\ell}(1)=e^{ia -\frac{1}{2}\sigma^2} .
    \end{equation*}
    Where, by the linearity of the Bochner integral,
    \begin{equation*}
        a       =\int_{E}\ell(x)\mu(dx)=\ell(m);\quad
        \sigma  =\int_{E}\ell(x)^2\mu(dx)=C_\mu (\ell ,\ell).
    \end{equation*}
    For the converse see \cite{hairer2009introduction} page $9$. Where it is proved that the Fourier transform characterises measures on separable Banach spaces.
\end{proof}




\subsection{Operators}
\begin{proposition}
    Let $X$ be a normed space, then the only open subspace of $X$ is itself.
\end{proposition}
\begin{proof}
    Let $U\subset X$ be an open subspace and choose $y\notin U$, then, by linearity, $\lambda_n y\notin U$ for all $\lambda_n\in\mathbb{K}$. Choose $\lambda_n\to 0$, then $\lambda_n x\to 0 \in U$. As a result $U^c$ is not closed which is a contradiction.
\end{proof}
\begin{proposition}
    Let $X, Y$ be normed spaces, then if $T\in\mathcal{L}(X,Y)$ is open it is also surjective
\end{proposition}
\begin{proof}
    $T(X)$ is open and a linear subspace.
\end{proof}
\begin{proposition}[Linear mapping theorem]
    Let $E_1,E_2$ be Banach spaces, then any surjective mapping $T\in\mathcal{L}(E_1,E_2)$ is open.
\end{proposition}
\begin{proposition}[Homeomorphism theorem]
    Let $E_1,E_2$ be Banach spaces, then $T\in\mathcal{L}(E_1,E_2)$ is a homeomorphism iff it is bijective.
\end{proposition}
\begin{proof}
    By the open mapping theorem a linear (continuous) operator is open iff it is surjective. Since $T$ is surjective it is open so $T^{-1}$ is continuous.
\end{proof}
\subsubsection{Compact operators}
Let $X,Y$ be normed spaces.
\begin{definition}
    We say that an operator $T$ is compact if it transforms bounded sets into relatively compact sets.
\end{definition}
Note that by definition any compact operator must be continuous as relatively compact sets must be bounded. However the reverse is not true, for example $T=Id$ on $l^2$. The reason for this is because $l^2$ does not have finite dimension however, since every bounded set in $\R^n$ is relatively compact we obtain the following.
\begin{proposition}
    Le $T:X\to Y$ have finite range, then $T$ is compact $\iff$ it is continuous.
\end{proposition}
\begin{proposition}
    The identity is relatively compact $\iff$ $X$ has finite dimension
\end{proposition}
\begin{proof}
    $\bar{B}(0,1)$ is compact $\iff$ the dimension of $X$ is finite.
\end{proof}
\begin{proposition}
    $T$ is compact $\iff T(B(0,r))$ is relatively compact for some $r\in\R\iff T(B(0,r))$
\end{proposition}
\begin{proof}
    The implication is clear, let $A\subset X$ be bounded. Then for some $r\in\R$
    \begin{equation*}
        T(A)\subset T(B(0,R))=\frac{R}{r} T(B(0,r))
    \end{equation*}
    Since any closed set within a compact set is compact and this concludes the proof.
\end{proof}
Let us write $\mathcal{K}(X,Y)$ for the space of relatively compact linear operators, then
\begin{proposition}
    Let $T:X\to Y$ be a linear operator, then
    \begin{itemize}
        \item $\mathcal{K}(X,Y)$ is a vector space.
        \item The composition of a continuous and compact operator is compact.
        \item An isomorphism is compact $\iff dim(X)<\infty$
    \end{itemize}
\end{proposition}
\begin{proof}
    The first two items can be proven using convergent sub-sequences. The second follows from considering $Id=T\circ T^{-1}$ together with the second item.
\end{proof}

\begin{proposition}
    Let $T:X\to Y$ with $Y$ complete. Then if $T_n$ are compact operators converging in $\mathcal{L}(X\to Y)$ to $T$ we have that $T$ is relatively compact.
\end{proposition}
\begin{proof}
    This follows from the fact that in complete normed spaces compactness is equivalent to total boundedness. So if we cover $T_N(A)$ by $B(y_n,\epsilon/2)$ then for large $N$ and $x\in A$
    \begin{equation*}
        \norm{T(x)-y_n}\leq \norm{T-T_N}\norm{x}-\norm{T_N(x)-y_n}<\epsilon
    \end{equation*}
\end{proof}
\begin{proposition}
    Let $T$ be a compact operator between normed spaces, then $T(X)$ is separable
\end{proposition}
\begin{proof}
    We have that $T(X)$ is the union of $T(B_n(0))$ which are separable as ther are relatively compact and thus totally bounded.
\end{proof}
\subsubsection{Fredholm alternative}
\begin{proposition}
    Let $T\in\mathcal{K}(E)$ with $E$ a Banach space, then $\ker(T-\lambda I)$ isa finite dimensional vector space for all $\lambda\in\mathbb{K}$.
\end{proposition}
\begin{proof}
    Since $T$ is compact so is $\frac{T}{\lambda}$, also when restricted to $\ker(T/\lambda-I)=\ker(T-\lambda I)$. But is is the identity here so this space must have finite. dimension
\end{proof}
\subsubsection{Invertible operators}
Let $E$ be a Banach space we recall that $\mathcal{L}(E)$ is a ring and that we say that $T\in\mathcal{L}(E)$ is invertible if $T^{-1}\in\mathcal{L}(E)$. Our next proposition states that, if an operator is not too far from the identity, then it is invertible.
\begin{proposition}
    If $T\in\mathcal{L}(E)$ is such that $\norm{T}<1$ then $I-T$ is invertible with inverse
    \begin{equation*}
        (I-T)^{-1}=\sum_{n=1}^\infty T^n
    \end{equation*}
    \begin{proof}
        This follows from the completeness of $\mathcal{L}(E)$ and the fact that the above series is normally convergent.
    \end{proof}
\end{proposition}
Using this we can prove that operators close to any invertible operator are invertible, where the closeness depends on how large $\norm{T^{-1}}$ is.
\begin{corollary}
    The open operators are an open set of $\mathcal{L}(E)$ for a Banach space $E$.
\end{corollary}
\begin{proof}
    Let $S\in\mathcal{L}(X)$  with $\norm{T-S}<1/\norm{T^{-1}}$, then it holds that
    \begin{equation*}
        \norm{ST^{-1}-Id}=\norm{(S-T)T^{-1}}\leq\norm{S-T}\norm{T^{-1}}<1
    \end{equation*}
    This proves that $ST^{-1}$ is invertible and thus, since $T$ is invertible so is $S$.
\end{proof}
\subsubsection{Specter of a linear operator}
\begin{definition}
    Let $T\in\mathcal{L}(E)$ with $E$ a Banach space, then we say that the \emph{specter of T} is
    \begin{equation*}
        \sigma(T):=\{\lambda\in\mathbb{K}: T-\lambda I \text{ is not invertible}\}
    \end{equation*}
\end{definition}
\begin{definition}
    Let $T\in\mathcal{L}(E)$ with $E$ a Banach space, then we say that the \emph{eigenvalues of T} are the $\lambda$ such that $T-\lambda I$ is not injective. We call the set of eigenvalues the \emph{pointwise specter} of $T$
    \begin{equation*}
        \sigma(T):=\{\lambda\in\mathbb{K}: T-\lambda I \text{ is not invertible}\}
    \end{equation*}
\end{definition}
For example the spectrum of $T=Id$ is $\sigma(T)=\{1\}$. Note that, by definition, the pointwise spectrum is within the spectrum.
\begin{proposition}
    Let $T\in\mathcal{L}(E)$, then $\sigma(T)$ is a closed subset in $\bar{B}(0,\norm{T})$
\end{proposition}
\begin{proof}
    Let $\lambda\notin \sigma(T)$, then $T-\lambda I$ is invertible so, since invertible operators are open set, so is  $T-(\lambda+\mu) I$ for small $\mu$. This shows that $\sigma(T)$ is closed. Furthermore, if $\lambda>\norm{T}$ then
    \begin{equation*}
        T-\lambda I=\lambda \qty(\frac{T}{\lambda}-I).
    \end{equation*}
    Since $\norm{T\lambda}<1$ this is invertible.
\end{proof}
\begin{definition}
    The spectral radius of $T$ is defined as
    \begin{equation*}
        \rho(T):=\sup\{\abs{\lambda:\lambda\in\sigma(T)}\}
    \end{equation*}
\end{definition}
\begin{proposition}
    Let $T:E_1\to E_2$ be an operator between two Banach spaces such that
    \begin{equation*}
        \norm{Tx}\geq c\norm{x}\quad\forall x\in E_1.
    \end{equation*}
    Then $T:X\to Im(T)$ is a homeomorhphism.
\end{proposition}
\begin{proof}
    $T$ is injective by the condition, it's image is closed as if $T(x_n)$ converges so does $x_n$ by the condition. As a result, $T:E_1\to Im(T)$ is a bijective operator between two Banach spaces so it is invertible by the homeomorphism theorem.
\end{proof}
\begin{proposition}
    Let $T$ be a non-zero compact operator on a Banach space, then nonzero elements $\lambda\in\sigma(T)$ are also eigenvectors with finite dimensional eigenspace $ker(T-\lambda I)$.
\end{proposition}
\begin{proof}
    By hypothesis $T-\lambda I$ is compact. If $\lambda$ is not an eigenvalue then $T-\lambda I$ is injective, so equivalently, by the Fredholm alternative it is surjective and thus, by the homeomorphism theorem, invertible. This contradicts that $\lambda$ is in the spectrum
\end{proof}
\subsection{Self-adjoint operators}
\begin{proposition}
    Let $T\in\mathcal{L}(H)$with $H$ a Hilbert space, then
    \begin{equation*}
        \ker(T)=(\Im T^*)^\perp
    \end{equation*}
\end{proposition}
\begin{proof}
    Let $e\in \ker(T)$, then $\br{e,T^*v}=0$. The other inclusion is due to $H^\perp =0$.
\end{proof}
\begin{proposition}
    Let $T$ be a self-adjoint operator on a Hilbert space, then its spectrum is non-empty (and contains $\norm{T}$ or $-\norm{T}$)
\end{proposition}

\begin{proposition}
    Let $T\in\mathcal{L}(H)$ be compact and self-adjoint, then if $D=\sigma_P\setminus\{0\}$ is the set of non-zero eigenvalues we have that
    \begin{equation*}
        H=\operatorname{ker}(T) \oplus \overline{\bigoplus_{\lambda \in D} \operatorname{ker}(T-\lambda I)}
    \end{equation*}
    Where $\operatorname{ker}(T-\lambda I)$ are finite dimensional spaces orthogonal amongst each other.
\end{proposition}
\begin{proof}
    Firstly we have that, since $T$ is compact, $\ker(T-\lambda I)$ is finite dimensional, and since it is self-adjoint they are orthogonal. Write $E$ for the term on rhs, by self-adjointness also
    \begin{equation*}
        \ker{T}\subset (\ker(T-\lambda I))^\perp
    \end{equation*}
    for all $\lambda\in D$ so $\ker{T}\subset E^\perp$  . To see the other inclusion we have that i$T|_{E^\perp}$ s compact and self adjoint, so if it is non-zero it has some nonzero eigenvalue ($\norm{T}$ or $-\norm{T}$) but all the non-zero eigenvalues are in $E$ not in $E^\perp$. So $E^\perp\subset\ker{T}$. As a result $E^\perp=\ker{T}$ so, since $E$ is a closed vector space,
    \begin{equation*}
        H=E^\perp\oplus {E^\perp}^\perp=\ker(T)\oplus E
    \end{equation*}
\end{proof}
\begin{theorem}[Hilbert Schmidt]\label{Hilber-Schmidt}
    Let $T\in\mathcal{L}(H)$ be a compact, self adjoint operator, then it diagonalizes. That is, there exists an orthonormal basis $e_i$ of eigenvectors with eigenvalues $\lambda_i\in\R$. Furthermore,
    \begin{equation*}
        Tx=\sum_{i\in I} \lambda_i\br{x,e_i}e_i\quad\forall x\in H.
    \end{equation*}
\end{theorem}
\begin{proof}
    We know that if $\sigma_P$ is the set of eigenvalues, then
    \begin{equation*}
        H= \overline{\bigoplus_{\lambda \in \sigma_P} \operatorname{ker}(T-\lambda I)}
    \end{equation*}
    where all the appearing spaces are orthogonal. It suffices to choose an orthonormal basis in each of $\operatorname{ker}(T-\lambda I)$.
\end{proof}
In the finite dimensional case we have that if $T$ is self adjoint and non-negative then there exists a diagonal matrix $D$ and a unitary matrix $U$ with
\begin{equation*}
    T=UDU^\dagger=(U\sqrt{D}U^\dagger)(U\sqrt{D}U^\dagger)
\end{equation*}
That is, $T$ has a square root which is itself non-negative and positive definite. The same result holds for separable Hilbert spaces.
\begin{theorem}
    Let $T$ be non-negative and self-adjoint on a separable Hilbert space, then there exists a unique non-negative self adjoint operator $\sqrt{T}$ such that $T=\sqrt{T}\sqrt{T}$.
\end{theorem}
This follows from the spectral theorem or representation of $C$ star algebras see \cite{lototsky2017stochastic} page 217. We note that given any operator $T\in \Ll(U,H)$ the operator $TT^*$ is non-negative and self-adjoint. As a result we can make the following definition.
\begin{definition}
    Given a linear operator $T$ between Hilbert spaces we define
    \begin{equation*}
        \abs{T}:=\sqrt{T T^*}  .
    \end{equation*}

\end{definition}

\subsection{Trace and Hilbert Schmidt operators}
We consider in this subsection $U,H$ to be separable Hilbert spaces, though much would remain valid for complex Hilbert spaces which are not necessarily separable with minimal adjustments.
\begin{definition}
    A linear operator $Q\in L(U)$ is said to be non-negative if
    \begin{equation*}
        \br{Qu,u}\geq 0\quad\forall u\in U.
    \end{equation*}
\end{definition}
\begin{definition}
    A linear operator $Q\in L(U)$ is said to be symmetric (or self-adjoint) if
    \begin{equation*}
        \br{Qu,v}=\br{u,Qv}\quad\forall u,v\in U.
    \end{equation*}
\end{definition}
\begin{definition}
    A partial isometry is a linear map $Q:U\to H$ such that $Q$ is an isometry on $Ker(Q)^\perp$.
\end{definition}
\begin{theorem}[Polar Theorem]
    Let $Q:U\to H$ be a linear operator between Hilbert spaces, then there is a unique partial isometry $T$ such that
    \begin{equation*}
        Q=T\abs{Q};\quad \ker(T)=\ker(Q).
    \end{equation*}
    Additionally $T^*Q=\abs{Q}$.
\end{theorem}
\begin{proof}
    Since $\norm{T}=\norm{\abs{T}}$ we may define $T(\abs{Q}x)=Qx$ and  $Ty=0$ for  $y_\in \ker{Q}$. This can then be extended continuously to $U=\overline{\Im(\abs{Q})}\oplus \Im(\abs{Q})^\perp$   See \cite{murphy2014c} page 51.
\end{proof}

The above theorem is the analogous of the fact that a complex number can be written as
\begin{equation*}
    z=e^{i\arg(z)}\abs{z}.
\end{equation*}


\begin{definition}
    A linear operator $Q\in L(U\to H)$ is said to have finite trace if for every orthonormal basis $\{e_k\}_{k=1}^\infty \subset  U$
    \begin{equation*}
        \norm{Q}_1:=\sum_{k\in\mathbb{N}}\br{\abs{Q}e_k,e_k}<\infty.
    \end{equation*}
    It is also said that $Q$ is trace class or nuclear. We write the space of such functions as $\mathcal{L}_1(U)$.
\end{definition}
Despite the name, we cannot define the trace of a trace class operator $Q$ unless  $U=H$. In this case we say that the trace of $Q$ is the quantity
\begin{equation*}
    Tr(Q):=\sum_{n=0}^{\infty} \br{Q e_n,e_n}.
\end{equation*}
We note that $Tr(Q)\leq \norm{Q}_1$ and that is is not enough for the trace of $Q$ to exist for $Q$ to be trace class as the sum may convergence but not absolutely. In the literature one may find many other equivalent definitions for trace class operators. We include the following list below which may be found in \cite{lototsky2017stochastic} page $92$.
\begin{proposition} Given two Hilbert spaces (not sure if separable would be necessary...) $U,H$ and a linear operator  $Q\in L(U,H)$ the following are equivalent.
    \begin{enumerate}
        \item $Q$ is trace class.
        \item For all orthonormal basis of $U,H$ we have $\sum_{n=0}^{\infty} \br{Qe_n,v_n}<\infty$.
        \item $Q$ is compact and  $\sum_{n=0}^{\infty} \lambda _n<\infty$ where $\lambda _n$ are the eigenvalues of $\abs{Q}$.
        \item $Q$ is nuclear, that is there exists  $\lambda _n \in \R,\ell_n \in H^*$ and $y_n \in H$ bounded sequences such that
              \begin{equation*}
                  T(x)=\sum_{n=0}^{\infty} \lambda _n\ell_n(x)y_n \quad\forall x\in H.
              \end{equation*}
        \item $Q^*$ is nuclear.
        \item $\sum_{n=0}^{\infty} \norm{Qe_n}<\infty$ for some orthonormal basis.

    \end{enumerate}
\end{proposition}

\begin{definition}
    We write $\mathcal{L}_1^+(U)$ for the space of trace class, positive semi-definite, symmetric operators.
\end{definition}
Though in infinite dimension this definition could depend on the basis it will not do so for the class of linear operators we are interested in. This is because due to the following result all these operators are nuclear.
\begin{proposition}[Hilbert Schmidt for $\mathcal{L}_1^+$]
    If $Q \in \mathcal{L}_1^+(U)$ then there exists an orthonormal basis $e_{k}, k \in \mathbb{N}$, of $U$ such that
    $$
        Q e_{k}=\lambda_{k} e_{k}, \quad \lambda_{k} \geq 0\quad \forall k \in \mathbb{N}.
    $$
\end{proposition}
\begin{proof}
    By definition of $mathcal{L}_1^+(U)$ we have that $Q$ is self-adjoint and compact (as it is nuclear).
    As a result we may apply the Hilbert Schmidt theorem \ref{Hilber-Schmidt} to obtain that
    \begin{equation*}
        Q x=\sum_{k\in\N}\lambda_{k} \br{x,e_{k}}.
    \end{equation*}
    Finally, $\lambda_k\geq 0$ as $Q$ is positive semi-definite.
\end{proof}
\subsection{Elliptic SPDEs}
In this section we define what is meant by a linear SPDE of the form $\Ll X-dW$. To do so we must first define white noise and thus generalized fields. The following definition is taken from \cite{lototsky2017stochastic} page 105
\begin{definition}
    A generalized random field over a topological vector space $V$ is a collection of random variables  $\{\mathcal{X}(u)\}_{u\in V}$ such that the following hold
    \begin{enumerate}
        \item \emph{Linearity}: $\mathcal{X}(u+av)=X(u)+aX(v)$ for all  $u,v\in V$ and $a\in \R$.
        \item \emph{Continuity}: If $u_n\to u\in V$ then we have convergence in probability $\mathcal{X}(u_n)\xrightarrow{\mathbb{P}}\mathcal{X}(u)$.
    \end{enumerate}
\end{definition}
A particular class of generalized fields are the Gaussian ones
\begin{definition}
    We say that a $0$ mean \emph{generalized Gaussian field} $\Bb$ over $H$ is a collection of real valued Gaussian random variables $\{\Bb(f)\}_{f\in H} $ such that
    \begin{enumerate}
        \item $\E[\Bb(f)]=0$ for all $f\in H$.
        \item There exists $K\in \Ll_1^+(H)$ such that
    \end{enumerate}
\end{definition}
\bibliography{biblio.bib}
\end{document}
